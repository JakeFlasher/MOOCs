{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using vectorization in Python\n",
    "\n",
    "This notebook demonstrates how to use vectorization in Python as well as comparisons of using vectorization and using for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "HOW_LONG_TO_RUN = 100\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized vs. For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250047.14194216448\n",
      "Loop version: 553.812026978 ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = 0\n",
    "for j in range(len(a)):\n",
    "    c += a[j] * b[j]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250047.14194217467\n",
      "Loop version: 9.2921257019 ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Vectorization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized(a, b, nb_times):\n",
    "    m = []\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        c = np.dot(a, b)\n",
    "        toc = time.time()\n",
    "        #print(\"Vectorized version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as vectorized version: 0.910415649414 ms\n",
      "Value: 250490.600103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = vectorized(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as vectorized version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_zip(a, b, nb_times):\n",
    "    m = []\n",
    "    c = 0\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        for a_j, b_j in zip(a, b):\n",
    "            c += a_j * b_j\n",
    "        toc = time.time()\n",
    "        #print(\"Zip version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as for loop version: 406.943519115 ms\n",
      "Value: 25049060.0103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = for_loop_zip(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as for loop version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop(a, b, nb_times):\n",
    "    m = []\n",
    "    c = 0\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        for j in range(len(a)):\n",
    "                c += a[j] * b[j]\n",
    "        toc = time.time()\n",
    "        #print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as for loop version: 349.924705029 ms\n",
      "Value: 25049060.0103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = for_loop(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as for loop version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other vectorizations in Python Numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: [0.14923004 0.68556227 0.85590332 ... 0.6686893  0.52930779 0.32877505]\n",
      "exponential: [1.16094003 1.98488756 2.35349938 ... 1.95167758 1.69775669 1.38926531]\n",
      "log: [-1.90226624 -0.37751595 -0.15559785 ... -0.40243575 -0.63618519\n",
      " -1.1123815 ]\n",
      "abs: [0.14923004 0.68556227 0.85590332 ... 0.6686893  0.52930779 0.32877505]\n",
      "maximum: [0.24038818 0.68556227 0.85590332 ... 0.89155843 0.52930779 0.32877505]\n",
      "square: [0.02226961 0.46999562 0.73257049 ... 0.44714538 0.28016673 0.10809303]\n"
     ]
    }
   ],
   "source": [
    "print('vector: {}'.format(a))\n",
    "u = np.exp(a) # exponential\n",
    "print('exponential: {}'.format(u))\n",
    "v = np.log(a)\n",
    "print('log: {}'.format(v))\n",
    "w = np.abs(a)\n",
    "print('abs: {}'.format(w))\n",
    "x = np.maximum(a, b)\n",
    "print('maximum: {}'.format(x))\n",
    "y = a**2\n",
    "print('square: {}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression with vectors\n",
    "\n",
    "Calculating for $m$ examples and $n = 2$ features <font color='red'>without</font> vectorization\n",
    "\n",
    "$J = 0;\\ \\ dw_1 = 0;\\ \\ dw_2 = 0;\\ \\ db = 0$<br>\n",
    "$\\text{For}\\ \\ i=1\\ \\ \\text{to}\\ \\ m $<br>\n",
    "$\\hspace{15pt}z^{(i)} = w^Tx^{(i)} + b$<br>\n",
    "$\\hspace{15pt}a^{(i)} = \\sigma(z^{(i)})$<br>\n",
    "$\\hspace{15pt}J\\ += - [y^{(i)}\\ log(a^{(i)}) + (1 - y^{(i)})\\ log(1 - a^{(i)})]$<br>\n",
    "$\\hspace{15pt}dz^{(i)} = a^{(i)} - y^{(i)}$<br>\n",
    "$\\hspace{15pt}\\text{For}\\ \\ j=1 \\ \\text{to} \\ \\ n$<br>\n",
    "$\\hspace{30pt}dw_j += x_j^{(i)}dz^{(i)}$<br>\n",
    "$\\hspace{15pt}\\text{End for}$<br>\n",
    "$\\hspace{15pt}db += dz^{(i)}$<br>\n",
    "$\\hspace{15pt}J\\ /=\\ m$<br>\n",
    "$\\hspace{15pt}dw_1\\ /=\\ m$<br>\n",
    "$\\hspace{15pt}dw_2\\ /=\\ m$<br>\n",
    "$\\hspace{15pt}db\\ /=\\ m$<br>\n",
    "$\\text{End for}$<br>\n",
    "\n",
    "### Updating weights\n",
    "\n",
    "$w_1\\ := w_1 - \\alpha dw_1$<br>\n",
    "$w_2\\ := w_2 - \\alpha dw_2$<br>\n",
    "$b\\ := b - \\alpha db$<br>\n",
    "\n",
    "The code in Python implementing this function is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize elements\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-1.*z))\n",
    "\n",
    "#hyperparameter\n",
    "lr = 0.1\n",
    "# input\n",
    "x = np.array([\n",
    "    [0.1, 0.1],\n",
    "    [0.2, 0.2],\n",
    "    [0.3, 0.3]\n",
    "])\n",
    "m = x.shape[0]\n",
    "y = np.array([1, 2, 3])\n",
    "# weights\n",
    "w = np.array([0.2, 0.3])\n",
    "n = w.shape[0]\n",
    "dw = np.zeros((n, 1))\n",
    "# bias\n",
    "b = np.array([1.0])\n",
    "db = np.zeros(b.shape)\n",
    "\n",
    "z = np.zeros((m, 1))\n",
    "dz = np.zeros((m, 1))\n",
    "a = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error: 0.0\n",
      "Error: -0.845841870694\n",
      "Initial weights: [0.2, 0.3]\n",
      "Updated weights: [0.23160057 0.33160057]\n"
     ]
    }
   ],
   "source": [
    "J, dw1, dw2, db = 0.0, 0.0, 0.0, 0.0\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(w, x[i]) + b\n",
    "    a[i] = sigmoid(z[i])\n",
    "    J += -(y[i] * math.log(a[i]) + (1 - y[i]) * math.log(1 - a[i]))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    for j in range(n):\n",
    "        dw[j] += x[i][j] * dz[i]\n",
    "    db += dz[i]\n",
    "J /= m\n",
    "for j in range(n):\n",
    "    dw[j] /= m\n",
    "db /= m\n",
    "    \n",
    "for j in range(n):\n",
    "    w[j] = w[j] - lr * dw[j]\n",
    "b = b - lr * db\n",
    "\n",
    "print('Initial error: 0.0')\n",
    "print('Error: {}'.format(J))\n",
    "print('Initial weights: [0.2, 0.3]')\n",
    "print('Updated weights: {}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression\n",
    "\n",
    "**Calculating for $m$ examples and $n = 2$ features <font color='red'>with</font> vectorization**\n",
    "\n",
    "Consider that we have `m` training examples. In training using for loops, we make a prediction on the first example by computing $Z^{(1)}$. Then compute the activations $a^{(1)}$ in this first example. Next, we make a prediction on the second training example by computing $Z^{(2)}$ and then computing the activations $a^{(2)}$. Next, we make a prediction on the third example, by computing $Z^{(3)}$ and then computing the activations $a^{(3)}$, and so on. And you might need to do this `m` times if you have `m` training examples.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z^{(1)} = w^Tx^{(1)}+b & \\ \\ \\ \\ \\ & z^{(2)} = w^Tx^{(2)}+b & \\ \\ \\ \\ \\ & z^{(3)} = w^Tx^{(3)}+b \\\\ \n",
    "a^{(1)} = \\sigma (z^{(1)}) & \\ \\ \\ \\ \\ & a^{(2)} = \\sigma (z^{(2)}) & \\ \\ \\ \\ \\ & a^{(3)} = \\sigma (z^{(3)}) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Here, we define a matrix capital `X` to be your training inputs, stacked together in different columns as illustred below. This matrix is a ($n_x \\times m$) matrix. So, this means that `X` is a $\\mathbb{R}^{n_x,m}$ dimensional matrix. \n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} (n_x, m)\\hspace{15pt} \\mathbb{R}^{n_x,m}\n",
    "$\n",
    "\n",
    "To compute $Z^{(1)}$, $Z^{(2)}$, $Z^{(3)}$ and so on, all in one step, we construct a ($1\\times M$) matrix (row vector) to compute them all at the same time. \n",
    "\n",
    "$Z = [ z^{(1)}\\ \\ z^{(2)}\\ \\ \\dots \\ \\ z^{(m)}]$<br>\n",
    "$Z = w^TX + \n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\ldots & b_m\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} \\text{where}\\ \\ \\ b \\rightarrow (1,m) \\ \\ \\ \\text{i.e.,} \\ \\ \\  b \\in \\mathbb{R}^{1,m} \\ \\ \\ \\text{dimension}$<br>\n",
    "$Z = np.dot(w^T, X) + b$<br>\n",
    "$A = [ a^{(1)}\\ \\ a^{(2)}\\ \\ \\dots\\ \\ a^{(m)}] = \\sigma(Z)$<br>\n",
    "\n",
    "Thus, we compute:\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\ldots & w_n\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b_1 & b_2 & \\ldots & b_m\n",
    "\\end{bmatrix}$<br>\n",
    "$Z = \\begin{matrix}\n",
    "[w^Tx^{(1)}+b_1 & w^Tx^{(2)}+b_2 & ...  & w^Tx^{(m)}+b_m] & \\hspace{15pt}\\rightarrow\\hspace{15pt} (1, m)\\hspace{15pt} \\mathbb{R}^{1,m} \\end{matrix}$\n",
    "\n",
    "In Python, we can perform this calculation using a single command as:\n",
    "\n",
    "```python\n",
    "Z = np.dot(w.T, X) + b\n",
    "```\n",
    "\n",
    "Where `b` can be a $\\mathbb{R}^{1}$ with numpy library performing \"broadcast\" of the elements. Stacking elements $a^{(1)}$, $a^{(2)}$, ... $a^{(m)}$ into a single row vector, we obtain the vector `A` as:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}\n",
    "a^{(1)} & a^{(2)} & \\ldots & a^{(m)}\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} (1, m)\\hspace{15pt} \\mathbb{R}^{1,m}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression's Gradient Output\n",
    "\n",
    "So, for the gradient computation, we computed $dz^{(1)}$ for the first example, which is $a^{(1)} - y^1$, $dz^{(2)}$ which is equal to $a^{(2)} - y^2$ and so on for all `m` training examples.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "dz^{(1)} = a^{(1)} - y^1 & \\ \\ \\ \\ \\ & dz^{(2)} = a^{(2)} - y^2 & \\ \\ \\ \\ \\ & dz^{(3)} = a^{(3)} - y^3 \\\\ \n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "When vectorizing, we define a new variable `dZ` that contains $dz^{(1)}$, $dz^{(2)}$, up to $dz^{(m)}$ as illustred below. \n",
    "\n",
    "$$\n",
    "dZ = \\begin{bmatrix}\n",
    "dz^{(1)} & dz^{(2)} & \\ldots & dz^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that, all the `dz` variables are stacked horizontally, generating a ($1, m$) matrix or alternatively a $\\mathbb{R}^{1,m}$ dimensional row vector. We know that `A` and `Y` are also matrices with variables stacked horizontally as:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a^{(1)} & a^{(2)} & \\ldots & a^{(m)} \\\\ \n",
    "\\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\ldots & y^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on these definitions, we can compute `dZ` as `A` minus `Y` because it's going to be equal to: \n",
    "\n",
    "$$\n",
    "dZ = A - Y $$\n",
    "$$\n",
    "dZ = \\begin{bmatrix}\n",
    "a^{(1)} - y^{(1)} & a^{(2)} - y^{(2)} & \\ldots & a^{(m)} - y^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to compute `db`, we sum all elements of `dz` and divide by the number of elements `m` as:\n",
    "\n",
    "$$\n",
    "db = \\frac{1}{m} \\sum_{i=1}^{m} dz^{(i)}\n",
    "$$\n",
    "\n",
    "In a vectorized implementation using python, we can run:\n",
    "\n",
    "```python\n",
    "db = 1./m * np.sum(dZ)\n",
    "```\n",
    "\n",
    "Finally, for computing `dw` in non-vectorized version, we have:\n",
    "\n",
    "$dw = 0$<br>\n",
    "$dw\\ += x^{(1)} dz^{(1)}$<br>\n",
    "$dw\\ += x^{(2)} dz^{(2)}$<br>\n",
    "$\\ldots$<br>\n",
    "$dw\\ += x^{(m)} dz^{(m)}$<br>\n",
    "$dw\\ /= m$\n",
    "\n",
    "On the other hand, with a vectorized implementation, we have:\n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} X dZ^T\n",
    "$$\n",
    "\n",
    "Representing each matrix of the equation, we have:\n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} \\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "dz^{(1)} \\\\\n",
    "dz^{(2)} \\\\\n",
    "\\ldots \\\\\n",
    "dz^{(m)}\n",
    "\\end{bmatrix} = \\frac{1}{m} \\begin{bmatrix}\n",
    "x^{(1)} dz^{(1)} & x^{(2)} dz^{(2)} & \\ldots & x^{(m)} dz^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which generates a ($1, m$) matrix. Finally, our algorithm to compute the derivatives becomes:\n",
    "\n",
    "$Z = w^{T}X + b \\hspace{15pt}\\text{# np.dot(w.T, X) + b}$<br>\n",
    "$A = \\sigma(Z)$<br>\n",
    "$dZ = A - Y$<br>\n",
    "$dw = \\frac{1}{m} X dZ^{T}$<br>\n",
    "$db = \\frac{1}{m} \\text{np.sum(dZ)}$\n",
    "\n",
    "And for update weights, we compute:\n",
    "\n",
    "$$w = w - \\alpha dw$$\n",
    "$$b = b - \\alpha db$$\n",
    "\n",
    "Below, there is an example of the Python implementation of this vectorized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize elements\n",
    "def sigmoid(Z):\n",
    "    return 1./(1. + np.exp(-1.*Z))\n",
    "\n",
    "#hyperparameter\n",
    "lr = 0.1\n",
    "nb_epochs = 10\n",
    "# input (nx, m) matrix 3 features, 4 examples\n",
    "X = np.array([\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "    [0.2, 0.2, 0.2, 0.2],\n",
    "    [0.3, 0.3, 0.3, 0.3]\n",
    "])\n",
    "n, m = X.shape\n",
    "Y = np.array([1, 2, 0, 2])\n",
    "\n",
    "# weights (1, n) matrix\n",
    "w = np.array([0.02, 0.03, 0.01])\n",
    "dw = np.zeros((1, m))\n",
    "# bias\n",
    "b = np.array([1.0])\n",
    "db = 0\n",
    "\n",
    "Z = np.zeros((1, m))\n",
    "dZ = np.zeros((1, m))\n",
    "A = np.zeros((1, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.718222810093\n",
      "Error: 0.709879917577\n",
      "Error: 0.702678351072\n",
      "Error: 0.696521727895\n",
      "Error: 0.691322332225\n",
      "Error: 0.687000366625\n",
      "Error: 0.683483249913\n",
      "Error: 0.680704964154\n",
      "Error: 0.678605451445\n",
      "Error: 0.677130059769\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    dZ = A - Y\n",
    "    dw = 1./m * np.dot(X, dZ.T)\n",
    "    db = 1./m * np.sum(dZ)\n",
    "    J = 1./m * (- np.dot(Y, np.log(A)) + np.dot((1 - Y), np.log(1 - A)))\n",
    "    \n",
    "    w = w - lr*dw\n",
    "    b = b - lr*db\n",
    "    print(\"Error: {}\".format(J))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting in Python\n",
    "\n",
    "In order to understand broadcasting in Python, we will consider as an example the matrix below. It shows the number of calories from carbohydrates, proteins, and fats in 100 grams of four different foods. So for example, a 100 grams of apples turns out, has 56 calories from carbs, and much less from proteins and fats. Whereas, in contrast, a 100 grams of beef has 104 calories from protein and 135 calories from fat. \n",
    "\n",
    "|     $-$ | Apples | Beef  | Eggs | Potatoes |\n",
    "| :------ | :----: | :---: | :--: | :------: |\n",
    "| Carb    |   56.0 |   0.0 |  4.4 |     68.0 |\n",
    "| Protein |    1.2 | 104.0 | 52.0 |      8.0 |\n",
    "| Fat     |    1.8 | 135.0 | 99.0 |      0.9 |\n",
    "\n",
    "Let's say your goal is to calculate the percentage of calories from carbs, proteins and fats for each of the four foods. For example, if you look at the first column and add up the numbers you get that 100 grams of apple has (56 + 1.2 + 1.8) = 59 calories. And so as a percentage the percentage of calories from carbohydrates in an apple would be 56 over 59, that's about 94.9%. \n",
    "\n",
    "Below, we have the Python code that implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n",
      "\n",
      "Total calories of each food:\n",
      "[ 59.  239.  155.4  76.9]\n",
      "\n",
      "Percentage of calories:\n",
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[56.0,   0.0,  4.4, 68.0],\n",
    "              [ 1.2, 104.0, 52.0,  8.0],\n",
    "              [ 1.8, 135.0, 99.0,  0.9]])\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "print('')\n",
    "cal = A.sum(axis=0)\n",
    "print('Total calories of each food:')\n",
    "print(cal)\n",
    "print('')\n",
    "percentage = 100*A/cal.reshape(1,4)\n",
    "print('Percentage of calories:')\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last command `percentage = 100*A/cal.reshape(1,4)` takes the matrix `A` and divided it by a (1,4) matrix, giving the matrix of percentages. Broadcasting allows us to perform such operation, *i.e.*, divide a (3,4) matrix by a (1,4) matrix. The process of broadcasting also occurs when we multiply an array by a scalar. For example, multiplying the matrix `B = [1, 2, 3]` by the scalar `c=2`, we obtain as result an array equals to `[2, 4, 6]` as presented in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication of numpy array by a scalar: [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([1,2,3])\n",
    "c = 2\n",
    "print('Multiplication of numpy array by a scalar: {}'.format(B*c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the application of broadcasting in Logistic Regression is using it to calculate `Z` when summing the bias with the matrix $w^TX$.\n",
    "\n",
    "# A Note on Numpy/Python Vectors\n",
    "\n",
    "Avoid Rank-1 arrays in Numpy. Rank-1 arrays are arrays that are neither row nor column vectors. In Python they are represented as `(value,)` array (*e.g*., `(5,)`). For example, consider we will create a random numpy array with 5 elements as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86885334 -0.56783931  0.15419041  0.56043105 -0.43289253]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, a Rank-1 array is neither a row nor column vector. Thus, when we try to transpose it, it does not change its form. For example, consider that we want to transpose the vector created before, it generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86885334 -0.56783931  0.15419041  0.56043105 -0.43289253]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "b = a.T\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid creating a Rank-1 array, we have to explictly specify the shape of the array. Thus, instead of creating the random numpy array as before, we create it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4436642 ]\n",
      " [ 0.05796461]\n",
      " [-0.73062291]\n",
      " [-0.75503853]\n",
      " [ 0.94046183]]\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5,1)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can transpose it and the column vector will become a row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4436642   0.05796461 -0.73062291 -0.75503853  0.94046183]]\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "b = a.T\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function for logistic regression\n",
    "\n",
    "In logistic regression, we have that the prediction $\\hat{y} is\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^TX + b)\\ \\ \\ \\ \\text{where}\\ \\ \\ \\ \\sigma(Z) = \\frac{1}{1+e^{-Z}}\n",
    "$$\n",
    "\n",
    "And we want to interpret $\\hat{y}$ as the $p(y=1|x)$, *i.e.*, the chance that $y = 1$ for a given set of input features $x$. Another way to say this is that\n",
    "\n",
    "$\n",
    "\\begin{matrix}\n",
    "\\text{if}\\ \\ \\ y=1 & p(y|x) = \\hat{y} \\\\\n",
    "\\text{if}\\ \\ \\ y=0 & p(y|x) = 1-\\hat{y} \\\\\n",
    "\\end{matrix}\n",
    "$\n",
    "\n",
    "Merging both equation into a single equation, we have:\n",
    "\n",
    "$$\n",
    "p(y|x) = \\hat{y}^y (1-\\hat{y})^{(1-y)}\n",
    "$$\n",
    "\n",
    "As the log function is a strictly monotonically increasing function, maximizing $\\log p(y|x)$ should give you a similar result as optimizing $p(y|x)$, and if you compute $\\log p(y|x)$, it is equal to $\\log \\hat{y}^y (1 - \\hat{y})^{(1 - y)}$. So, that simplifies to:\n",
    "\n",
    "$$\n",
    "\\log p(y|x) = y \\log \\hat{y} + (1-y) \\log (1 - \\hat{y}) \\\\\n",
    "\\log p(y|x) = - \\mathcal{L}(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Taking into account all the examples ($m$) in the training set, we have the cost:\n",
    "\n",
    "$\\log p(\\text{labels in training set}) = \\log \\prod_{i=1}^{m} p(y^{(i}|x^{(i)})$<br>\n",
    "$\\log p(\\text{labels in training set}) = \\sum_{i=1}^{m} \\log p(y^{(i}|x^{(i)})$<br>\n",
    "$\\log p(\\text{labels in training set}) = - \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$\n",
    " \n",
    "\n",
    "As we want to minimize the cost, *i.e.*, maximize the log likelihood, we have:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vectorization cannot be done without a GPU.<br>\n",
    "\n",
    "&#9745; False<br>\n",
    "&#9744; True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the dimensions of matrix `X` in this video?<br>\n",
    "\n",
    "&#9744; ($m, n_x$)<br>\n",
    "&#9745; ($n_x, m$)<br>\n",
    "&#9744; ($m, $)<br>\n",
    "&#9744; ($m, 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you compute the derivative of `b` in one line of code in Python numpy?<br>\n",
    "\n",
    "&#9745; `1 / m*(np.sum(dz))`<br>\n",
    "&#9744; `1 - m(np.sum(dz))`<br>\n",
    "&#9744; `m(np.sum(dz))`<br>\n",
    "&#9744; `1 * m(np.sum(dz))`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which of the following numpy line of code would sum the values in a matrix `A` vertically?<br>\n",
    "\n",
    "&#9744; `A.sum(axis )`<br>\n",
    "&#9745; `A.sum(axis = 0)`<br>\n",
    "&#9744; `A.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What kind of array has dimensions in this format: `(10, )`?<br>\n",
    "\n",
    "&#9744; An identity array<br>\n",
    "&#9745; A rank 1 array<br>\n",
    "&#9744; A rank 0 array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. True or False: Minimizing the loss corresponds with maximizing $\\log p(y|x)$.\n",
    "\n",
    "&#9744; False<br>\n",
    "&#9745; True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
