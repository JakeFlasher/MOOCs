{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Overview\n",
    "\n",
    "From previous lessom, we implemented a logistic regression. In that model, we saw that we compute $z$ with features $x$ and parameters $w$ and $b$. $z$ is then used to computes $a$, which is used to predict $\\hat{y}$. Then you can compute the loss function $\\mathcal{L}$. \n",
    "\n",
    "<img src=\"https://cdn.rawgit.com/rogergranada/MOOCs/master/Coursera/Deeplearning.ai/Neural%20Networks%20and%20Deep%20Learning/Week%203/images/logistic_regression.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "A neural network is similar to the logistic regression, but in addition it stacking together a lot of little sigmoid units. In the neural network, the stack of nodes performs the $z$ calculation, as well as, the $a$ calculation. Thus, a node corresponds to a $z$ score and another node to another $z$ score. As logistic regression, we have as inputs the features $x$ and some parameters $w$ and $b$. For each node that calculate the values of $z$ and $a$, we define an index or identifier (a superscript square bracket to refer to quantities associated with this stack of nodes, also called the layer). Thus, a value $W^{[1]}$ corresponds to the scores in nodes of the first layer, $W^{[2]}$ the scores of the second layer and so on. The image below illustrates the architecture of a neural network.\n",
    "\n",
    "<img src=\"https://cdn.rawgit.com/rogergranada/MOOCs/master/Coursera/Deeplearning.ai/Neural%20Networks%20and%20Deep%20Learning/Week%203/images/neural_network.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "# Neural Network Representation\n",
    "\n",
    "To introduce neural networks, we start by showing the neural network with a single hidden layer, as illustred in the image below. \n",
    "\n",
    "<img src=\"https://cdn.rawgit.com/rogergranada/MOOCs/master/Coursera/Deeplearning.ai/Neural%20Networks%20and%20Deep%20Learning/Week%203/images/neural_network_explanation.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "This network contains input features ($x_1, x_2, x_3$) stacked up vertically in the so called `input layer`. In our image, the input is an array $X$ containing three rows and one column. \n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For input, we use the vector $X$ to denote the input features, but an alternative notation for the values of the input features will be $a^{[0]}$. And the term `a` stands for activations and it refers to the values that different layers of the neural network are passing on to the subsequent layers. Thus, the input layer passes on the values of $X$ to the hidden layer, and the activations of the input layer will be called $a^{[0]}$.\n",
    "\n",
    "The next layer containing four nodes, is called the `hidden layer` of the neural network. It is called hidden layer because in the training set, the true values for these nodes in the middle are not observed. The input values and the output can be seen, but things in this layer are not seen in the training set. This layer will generate some set of activations, which will be called $a^{[1]}$. In particular, the first unit or the first node generates a value $a^{[1]}_1$, the second node generates a value $a^{[1]}_2$, and so on. This layer is a four dimension vector (4 column vector) since we have four nodes, or four units, or four hidden units in this hidden layer, as:\n",
    "\n",
    "$$\n",
    "a^{[1]} = \\begin{bmatrix}\n",
    "a^{[1]}_1 \\\\\n",
    "a^{[1]}_2 \\\\\n",
    "a^{[1]}_3 \\\\\n",
    "a^{[1]}_4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, the last layer is a single-node layer called `output layer`. It is responsible for generating the predicted value $\\hat{y}$. In a neural network that you train with supervised learning, the training set contains values of the inputs $X$ as well as the target outputs $Y$. This value generates some value $a^{[2]}$, which is just a real number. Thus, $\\hat{y}$ takes on the value of $a^{[2]}$. \n",
    "\n",
    "This kind of neural network illustred in the image is called *two-layer neural network* and is called as two-layer because we count all layers in neural networks but the input layer. Hence, the hidden layer is layer one and the output layer is layer two. In our notational convention, the input layer is the layer zero, so technically, there are three layers in this neural network since there is the input layer, the hidden layer, and the output layer. \n",
    "\n",
    "Finally, the hidden layer and the output layers will have parameters associated with them. Each layer will have associated with it parameters $w$ and $b$. We write $w^{[1]}$ to indicate that these are parameters associated with layer one (the hidden layer). The weights matrix is a 4 by 3 matrix and b is 4 by 1 vector in this example, as:\n",
    "\n",
    "$$\n",
    "w^{[1]} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43} \\\\\n",
    "\\end{bmatrix} \\ \\ \\ \\ \\ \\ b^{[1]} = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where the first dimension (4) comes from the fact that we have four nodes or four hidden units, and the second dimension (3) comes from the fact that we have three input features. On the other hand, our matrix of parameters $w^{[2]}$ and $b^{[2]}$ contain a 1 by 4 and 1 by 1 matrices respectively. The 1 by 4 comes from the fact that the hidden layer has four hidden units and the output layer has just one unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Neural Network's Output\n",
    "\n",
    "Using the neural network presented above (2 layer neural network), we have to compute for each node the values of `z` and `a` (*e.g.*, $z^{[1]}_1$, $z^{[1]}_2$, $z^{[1]}_3$ and $z^{[1]}_4$). Considering the first node of layer one, we can compute the value of $z^{[1]}_1$ as:\n",
    "\n",
    "$$\n",
    "z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "a^{[1]}_1 = \\sigma(z^{[1]}_1)\n",
    "$$\n",
    "\n",
    "Extending the computation for all nodes of the hidden layer, we have:\n",
    "\n",
    "$$\n",
    "z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1,\\ \\ \\ \\ a^{[1]}_1 = \\sigma(z^{[1]}_1) \\\\\n",
    "z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2,\\ \\ \\ \\ a^{[1]}_2 = \\sigma(z^{[1]}_2) \\\\\n",
    "z^{[1]}_3 = w^{[1]T}_3 x + b^{[1]}_3,\\ \\ \\ \\ a^{[1]}_3 = \\sigma(z^{[1]}_3) \\\\\n",
    "z^{[1]}_4 = w^{[1]T}_4 x + b^{[1]}_4,\\ \\ \\ \\ a^{[1]}_4 = \\sigma(z^{[1]}_4) \\\\\n",
    "$$\n",
    "\n",
    "As computing them separately would be very inefficient, we can compute them using a vectorized version as:\n",
    "\n",
    "$$\n",
    "Z^{[1]} = W^{[1]T} X + b^{[1]} \\\\\n",
    "\\begin{bmatrix}\n",
    "z^{[1]}_1 \\\\\n",
    "z^{[1]}_2 \\\\\n",
    "z^{[1]}_3 \\\\\n",
    "z^{[1]}_4 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "- w^{[1]T}_1 - \\\\\n",
    "- w^{[1]T}_2 - \\\\\n",
    "- w^{[1]T}_3 - \\\\\n",
    "- w^{[1]T}_4 - \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b^{[1]}_1 \\\\\n",
    "b^{[1]}_2 \\\\\n",
    "b^{[1]}_3 \\\\\n",
    "b^{[1]}_4 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "w^{[1]T}_2 x + b^{[1]}_2 \\\\\n",
    "w^{[1]T}_3 x + b^{[1]}_3 \\\\\n",
    "w^{[1]T}_4 x + b^{[1]}_4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then, we compute $a^{[1]}$ as:\n",
    "\n",
    "$$\n",
    "a^{[1]} = \\sigma(z^{[1]}) \\\\\n",
    "\\begin{bmatrix}\n",
    "a^{[1]}_1 \\\\\n",
    "a^{[1]}_2 \\\\\n",
    "a^{[1]}_3 \\\\\n",
    "a^{[1]}_4 \\\\\n",
    "\\end{bmatrix} = \\sigma \\left ( \\begin{bmatrix}\n",
    "z^{[1]}_1 \\\\\n",
    "z^{[1]}_2 \\\\\n",
    "z^{[1]}_3 \\\\\n",
    "z^{[1]}_4\n",
    "\\end{bmatrix}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "Expanding the computation for all layers of our network, we have the computation and the dimensions as:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z^{[1]} = W^{[1]}x + b^{[1]} & (4,1) = (4,3)(3,1) + (4,1) \\\\\n",
    "a^{[1]} = \\sigma(z^{[1]})    & (4,1) = (4,1) \\\\\n",
    "z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} & (1,1) = (1,4)(4,1) + (1,1) \\\\\n",
    "a^{[2]} = \\hat{y} = \\sigma(z^{[2]})    & (1,1) = (1,1) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "An example in Python of this example is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.32177144]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-1.*z))\n",
    "\n",
    "# Input (3, 1) matrix\n",
    "X = np.array([\n",
    "    [0.1],\n",
    "    [0.2],\n",
    "    [0.3]\n",
    "])\n",
    "\n",
    "# Initialize matrices\n",
    "W1 = np.random.randn(3,4)\n",
    "W2 = np.random.randn(4,1)\n",
    "b1 = np.ones((4, 1))\n",
    "b2 = np.ones((1, 1))\n",
    "\n",
    "# Compute scores\n",
    "Z1 = np.dot(W1.T, X) + b1\n",
    "a1 = sigmoid(Z1)\n",
    "Z2 = np.dot(W2.T, a1) + b2\n",
    "a2 = sigmoid(Z2)\n",
    "yhat = a2\n",
    "print('Prediction: {}'.format(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "&#9744; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
