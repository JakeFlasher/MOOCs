{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep L-Layer Neural Network\n",
    "\n",
    "Here, we call deep neural network, a network that has at least 2 layers. A \"shallow\" network such as logistic regression contains a single layer as illustred in the image below:\n",
    "\n",
    "<img src=\"images/logistic_regression.svg\" width=\"30%\" align=\"center\"/>\n",
    "\n",
    "Consider the deep neural network with 4 layers illustred in the image below:\n",
    "\n",
    "<img src=\"images/deep_neural_network.svg\" width=\"50%\" align=\"center\"/>\n",
    "\n",
    "In this deep network, we have 4 layers (identified as $L=4$) and the number of neurons in each layer is identified as $n^{[l]}$, where $l$ is the layer. We index the input of the network as layer zero ($l=0$), the first hidden layer ($l=1$), the second hidden layer ($l=2$), the third hidden layer ($l=3$), and the output ($l=4$). Thus, we have $n^{[1]}=5$ since we have 5 units in layer 1, $n^{[2]}=5$ since we have 5 units in layer 2, $n^{[3]}=3$ since we have 3 units in layer 3, and $n^{[4]}=n^{[L]}=1$ since we have 1 units in the last layer. For the input, we have that $n^{[0]}=3$ since we have 3 features in the input. \n",
    "\n",
    "We also use $a^{[l]}$ to denote the activation in layer $l$. Thus, in forward, for example, we have that $a^{[l]}=g^{[l]}(z^{[l]})$. We use $w^{[l]}$ to denote the weights in layer $l$ and $b^{[l]}$ to denote the bias. Finally, we denote $X=a^{[0]}$ and $\\hat{y}=a^{[L]}$.\n",
    "\n",
    "\n",
    "# Forward Propagation in a Deep Network\n",
    "\n",
    "Considering the deep network illustred above, we can compute its forward propagation as:\n",
    "\n",
    "$\n",
    "Z^{[1]} = W^{[1]}X + b^{[1]} \\\\\n",
    "A^{[1]} = g^{[1]}(Z^{[1]}) \\\\\n",
    "Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "A^{[2]} = g^{[2]}(Z^{[2]}) \\\\\n",
    "Z^{[3]} = W^{[3]}A^{[2]} + b^{[3]} \\\\\n",
    "A^{[3]} = g^{[3]}(Z^{[3]}) \\\\\n",
    "Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]} \\\\\n",
    "A^{[4]} = g^{[4]}(Z^{[4]}) \\\\\n",
    "$\n",
    "\n",
    "Considering that $X=A^{[0]}$, we can generalize the equation to:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]}) \\\\\n",
    "$$\n",
    "\n",
    "Where the prediction is computed as:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = A^{[L]} = g^{[L]}(Z^{[L]})\n",
    "$$\n",
    "\n",
    "# Getting your matrix dimensions right\n",
    "\n",
    "Consider the 5-layer neural network illustred below:\n",
    "\n",
    "<img src=\"images/deep_neural_network_5layers.svg\" width=\"60%\" align=\"center\"/>\n",
    "\n",
    "In this network, we have the number of units as $n^{[0]} = n_x = 2$, $n^{[1]} = 3$, $n^{[2]} = 5$, $n^{[3]} = 4$, $n^{[4]} = 2$, and $n^{[5]} = 1$. The dimensions for each layer using a single example are defined as:\n",
    "\n",
    "$\n",
    "z^{[1]} = W^{[1]} X + b^{[1]} \\\\\n",
    "(3, 1) = (3, 2) (2, 1) + (3, 1) \\\\\n",
    "(n^{[1]}, 1) = (n^{[1]}, n^{[0]}) (n^{[0]}, 1) + (n^{[1]}, 1) \\\\\n",
    "$\n",
    "\n",
    "Using this example, we can see that $W^{[1]} : (n^{[1]}, n^{[0]})$ and in more general terms, we have $W^{[l]} : (n^{[l]}, n^{[l-1]})$. Considering the second layer as example, we can see that:\n",
    "\n",
    "$\n",
    "z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\\\\n",
    "(5, 1) = (5, 3) (3, 1) + (5, 1) \\\\\n",
    "(n^{[2]}, 1) = (n^{[2]}, n^{[1]}) (n^{[1]}, 1) + (n^{[2]}, 1) \\\\\n",
    "$\n",
    "\n",
    "As we can see for bias, $b^{[1]} = (n^{[1]}, 1)$, $b^{[2]} = (n^{[2]}, 1)$. In the general case, $b^{[l]} = (n^{[l]}, 1)$. When considering a vectorized implementation, our matrices $z$ and $a$ become $Z$ and $A$ with dimensions:\n",
    "\n",
    "$\n",
    "Z^{[1]} = W^{[1]} X + b^{[1]} \\\\\n",
    "(n^{[1]}, m) = (n^{[1]}, n^{[0]}) (n^{[0]}, m) + (n^{[1]}, 1) \\\\\n",
    "$\n",
    "\n",
    "where $b^{[1]}$ contains a single column but is broadcasted to $m$ examples, becoming $(n^{[1]}, m)$ automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why deep representations?\n",
    "\n",
    "Consider the problem of recognizing or detecting faces using a deep neural network. In this problem, the input of the network could be a picture of a face. From this input, the first layer of the neural network would be a feature detector or an edge detector, as illustred in the image below, where each little visualization represents a hidden unit that's trying to figure out where the edges of that orientation are in the image. The next layer of the neural network will group these edges in more complex forms and maybe identify parts of faces. For example, it might have a neuron trying to find an eye, and a different neuron trying to find a nose. By putting together lots of edges, it can start to detect different parts of faces. Finally, in deeper layers, by putting together different parts of faces, like an eye or a nose or an ear or a chin, it can recognize or detect different types of faces. \n",
    "\n",
    "<img src=\"images/deep_network.svg\" width=\"80%\" align=\"center\"/>\n",
    "\n",
    "From circuit theory, there is also the intuition about why deep networks seem to work well. The intuition says that you can compute a function with a relatively small (*i.e.*, the number of hidden units is relatively small) but deep neural network. On the other hand, if you try to compute the same function with a shallow network, then you might require exponentially more hidden units to compute the same function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of deep neural networks\n",
    "\n",
    "Consider a layer $l$ with $W^{[l]}$ and $b^{[l]}$, we compute the forward as:\n",
    "\n",
    "Input: $\\ \\ \\ a^{[l-1]}$<br>\n",
    "Output:$\\ \\ \\ a^{[l]}$<br>\n",
    "Compute:$\\ \\ \\ z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\\ \\ \\ $#cache$\\ \\ \\ z^{[l]}$<br>\n",
    "$\\hspace{40pt} a^{[l]} = g^{[l]}(z^{[l]})$<br>\n",
    "\n",
    "And the backward as:\n",
    "\n",
    "Input:$\\ \\ \\ da^{[l]} \\ \\ \\ \\text{and}\\ \\ \\ z^{[l]}$<br>\n",
    "Output:$\\ \\ \\ da^{[l-1]}, dW^{[l]}, db^{[l]}$<br>\n",
    "\n",
    "Considering the network presented previously, we select a layer $l$ with some parameters $w^{[l]}$ and $b^{[l]}$ as illustrated in the image below. For the forward propagation, we input the activations $a^{[l-1]}$ from your previous layer and output $a^{[l]}$. To do so, we compute $z^{[l]}$ and then $a^{[l]}$. It turns out that for later use it is useful to also cache the value $z^{[l]}$. For the backward step focusing on computation for this layer $l$, we implement a function that inputs $da^{[l]}$ and outputs $da^{[l-1]}$.\n",
    "\n",
    "<img src=\"images/building_blocks.svg\" width=\"30%\" align=\"center\"/>\n",
    "\n",
    "If you want to implement these functions, then the basic computation of the neural network will be as illustred in the image below. First, you have to take the input features $a^{[0]}$ and compute the activations of the first layer ($a^{[1]}$). To do that, you need the $w^{[1]}$ and $b^{[1]}$, and for future use, cache away $z^{[1]}$. Now, you feed that to the second layer and use $w^{[2]}$ and $b^{[2]}$ to compute the activations in the next layer $a^{[2]}$, and so on. Repeat this process until you end up outputting a $l$ which is equal to $\\hat{y}$.\n",
    "\n",
    "<img src=\"images/building_blocks_all_layers.svg\" width=\"80%\" align=\"center\"/>\n",
    "\n",
    "For the back propagation step, you have to perform a backward sequence of iterations in which you are going backwards and computing gradients like so. In order to do that, you feed $da^{[l]}$ and outputs $da^{[l-1]}$, and so on until we get $da^{[2]}$ and $da^{[1]}$. Along the way, back propagation also ends up outputting the derivatives for weights and bias ($dw^{[l]}$ and $db^{[l]}$ for all layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation \n",
    "\n",
    "In order to compute the forward and backward propagation, we use the following equations:\n",
    "\n",
    "**Forward Propagation** \n",
    "\n",
    "$\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]})\n",
    "$\n",
    "\n",
    "**Backward Propagation:**\n",
    "\n",
    "$\n",
    "dZ^{[l]} = A^{[l]} - Y \\\\\n",
    "dZ^{[l-1]} = W^{[l]^T}dZ^{[l]}*g'^{[l-1]}(Z^{[l-1]}) \\\\\n",
    "dA^{[l]} = W^{[l]^T}dZ^{[l]} \\\\\n",
    "dZ^{[l-1]} = dA^{[l]} * g'^{[l-1]}(Z^{[l-1]}) \\\\\n",
    "dW^{[l]} = \\frac{1}{m} dZ^{[l]}A^{[l-1]^T} \\\\\n",
    "db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, \\text{axis}=1, \\text{keepdims=True}) \\\\\n",
    "$\n",
    "\n",
    "where * is a element-wise multiplication. In the last layer of the network, we have in the forward (and for the case of logistic regression):\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) = -y \\log a - (1 - y) \\log (1 - a)$$\n",
    "\n",
    "And in the backward propagation:\n",
    "\n",
    "$$dA^{[l]} = \\left [ \\left (-\\frac{y^{[1]}}{a^{[1]}} + \\frac{1-y^{[1]}}{1-a^{[1]}}\\right ) \\ldots \\left (-\\frac{y^{[m]}}{a^{[m]}} + \\frac{1-y^{[m]}}{1-a^{[m]}} \\right) \\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Deep Neural Network\n",
    "\n",
    "Below we illustrate an example of a 3-layer deep neural network. In the image below, $W$ are the weights, $Z$ represents the computation of $W^TA + b$ and $A$ is the activation function. Symbols represented with $d$ are the derivations of each symbol.\n",
    "\n",
    "<img src=\"images/example_neural_network.svg\" width=\"50%\" align=\"center\"/>\n",
    "\n",
    "In this example, we first perform the forward propagation by first computing $Z$ and then apply the activation function $A$ as:\n",
    "\n",
    "**Forward propagation**:\n",
    "\n",
    "$\n",
    "Z^{[1]} = W^{[1]}X + b^{[1]} \\\\\n",
    "A^{[1]} = g(Z^{[1]}) \\ \\ \\ \\rightarrow \\ \\ \\ ReLU(Z^{[1]}) \\\\\n",
    "Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "A^{[2]} = g(Z^{[2]}) \\ \\ \\ \\rightarrow \\ \\ \\ ReLU(Z^{[2]}) \\\\\n",
    "Z^{[3]} = W^{[3]}A^{[2]} + b^{[3]} \\\\\n",
    "A^{[3]} = g(Z^{[3]}) \\ \\ \\ \\rightarrow \\ \\ \\ Sigmoid(Z^{[3]}) \\\\\n",
    "$\n",
    "\n",
    "Calculating the forward propagation, we obtain $\\hat{Y}$ as $A^{[3]}$. Having the prediction $\\hat{Y}$ we can calculate the loss (or the error) using:\n",
    "\n",
    "$\n",
    "J = \\frac{1}{m} (-Y \\log(\\hat{Y})^T + (1 - Y) \\log(1 - \\hat{Y})^T)\n",
    "$\n",
    "\n",
    "where $m$ is the number of examples we have in our dataset. As we have the true labels in $Y$, we can also calculate the derivative for $Z^{[3]}$, which is the same as calculating the derivative for $\\hat{Y}$. Having the derivative of $Z^{[3]}$, we can calulate the derivative of the weights and bias of the last layer. To perform such computation, we use:\n",
    "\n",
    "$\n",
    "dZ^{[3]} = -\\frac{Y}{\\hat{Y}} + \\frac{1-Y}{1-\\hat{Y}} \\\\\n",
    "dW^{[3]} = \\frac{1}{m} dZ^{[3]}A^{[2]^T} \\\\\n",
    "db^{[3]} = \\frac{1}{m} \\sum_{cols}dZ^{[3]}\n",
    "$\n",
    "\n",
    "As we calculated $dZ^{[3]}$, we can continue the backpropagation to the previous layer using:\n",
    "\n",
    "$\n",
    "dA^{[3]} = W^{[3]^T} dZ^{[3]} \\\\\n",
    "dZ^{[2]} = dA^{[3]} g'^{[2]}(Z^{[2]}) \\\\\n",
    "dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]^T} \\\\\n",
    "db^{[2]} = \\frac{1}{m} \\sum_{cols}dZ^{[2]}\n",
    "$\n",
    "\n",
    "And finally to the first layer using:\n",
    "\n",
    "$\n",
    "dA^{[2]} = W^{[2]^T} dZ^{[2]} \\\\\n",
    "dZ^{[1]} = dA^{[2]} g'^{[1]}(Z^{[1]}) \\\\\n",
    "dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^T \\\\\n",
    "db^{[1]} = \\frac{1}{m} \\sum_{cols}dZ^{[1]}\n",
    "$\n",
    "\n",
    "Now that we calculated all the derivatives, we can update all the weights and bias using a learning rate $\\alpha$ with the following:\n",
    "\n",
    "$\n",
    "W^{[1]} = W^{[1]} - \\alpha * dW^{[1]} \\\\\n",
    "b^{[1]} = b^{[1]} - \\alpha * db^{[1]} \\\\\n",
    "W^{[2]} = W^{[2]} - \\alpha * dW^{[2]} \\\\\n",
    "b^{[2]} = b^{[2]} - \\alpha * db^{[2]} \\\\\n",
    "W^{[3]} = W^{[3]} - \\alpha * dW^{[3]} \\\\\n",
    "b^{[3]} = b^{[3]} - \\alpha * db^{[3]}\n",
    "$\n",
    "\n",
    "A Python code performing all these computations for a toy example is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of deep neural network\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(z, grad=True):\n",
    "    a = np.maximum(0, z)\n",
    "    if grad:\n",
    "        da = np.where(z <= 0, 0, 1)\n",
    "        return a, da\n",
    "    return a\n",
    "\n",
    "\n",
    "def sigmoid(z, grad=True):\n",
    "    a = 1./(1. + np.exp(-1.*z))\n",
    "    if grad:\n",
    "        da = a*(1 - a)\n",
    "        return a, da\n",
    "    return a\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, X, Y, dims):\n",
    "        self.nn = {}\n",
    "        self.dv = {}\n",
    "        self.cache = {}\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.nb_layers = len(dims)\n",
    "        self.loss = float('inf')\n",
    "        n_prev = X.shape[0]\n",
    "        for l, n in enumerate(dims, start=1):\n",
    "            Wl = np.random.randn(n, n_prev)\n",
    "            dWl = np.zeros(Wl.shape)\n",
    "            bl = np.ones((n, 1))\n",
    "            dbl = np.zeros(bl.shape)\n",
    "            self.nn[l] = {'W': Wl, 'b': bl}\n",
    "            self.dv[l] = {'dW': dWl, 'db': dbl}\n",
    "            n_prev = n\n",
    "            \n",
    "    def g(self, Z, func='relu', grad=True):\n",
    "        # Activation functions\n",
    "        if func == 'relu':\n",
    "            return relu(Z, grad)\n",
    "        return sigmoid(Z, grad)\n",
    "            \n",
    "    def forward(self):\n",
    "        for l in sorted(self.nn):\n",
    "            Wl = self.nn[l]['W']\n",
    "            bl = self.nn[l]['b']\n",
    "            if l == 1:\n",
    "                #print 'W[1]X + b[1] : ', \n",
    "                #print '({},{})({},{}) + ({},{})'.format(Wl.shape[0],Wl.shape[1],self.X.shape[0],self.X.shape[1],bl.shape[0],bl.shape[1])\n",
    "                Zl = np.dot(Wl, self.X) + bl\n",
    "            else:\n",
    "                A_prev = self.cache[l-1]['A']\n",
    "                #print 'W[{}]A[{}] + b[{}] : '.format(l, l-1, l),\n",
    "                #print '({},{})({},{}) + ({},{})'.format(Wl.shape[0],Wl.shape[1],A_prev.shape[0],A_prev.shape[1],bl.shape[0],bl.shape[1])\n",
    "                Zl = np.dot(Wl, A_prev) + bl\n",
    "            if l == len(self.nn):\n",
    "                #print 'Sigmoid Z[{}] : '.format(l),\n",
    "                #print '({}, {})'.format(Zl.shape[0], Zl.shape[1])\n",
    "                Al, dAl = self.g(Zl, func='sigmoid')\n",
    "            else:\n",
    "                #print 'ReLU Z[{}] : '.format(l),\n",
    "                #print '({}, {})'.format(Zl.shape[0], Zl.shape[1])\n",
    "                Al, dAl = self.g(Zl, func='relu')\n",
    "            self.cache[l] = {'Z': Zl, 'A': Al, 'dg': dAl}\n",
    "        loss = 1./X.shape[1] * (- np.dot(Y, np.log(Al).T) + np.dot((1 - Y), np.log(1 - Al).T))\n",
    "        self.loss = loss[0][0]\n",
    "        return Al\n",
    "        \n",
    "    def backward(self):\n",
    "        drv = {}\n",
    "        for l in range(self.nb_layers, 0, -1):\n",
    "            #print 'Layer: ', l\n",
    "            if l == self.nb_layers:\n",
    "                Yhat = self.cache[self.nb_layers]['A']\n",
    "                dZl = -Y/Yhat + ((1 - Y)/(1 - Yhat))\n",
    "                #print 'dZ[{}] : ({}, {})'.format(l, dZl.shape[0], dZl.shape[1])\n",
    "                #print '({},{})({},{}) + ({},{})'.format(Wl.shape[0],Wl.shape[1],A_prev.shape[0],A_prev.shape[1],bl.shape[0],bl.shape[1])\n",
    "            else:\n",
    "                Wlt = self.nn[l+1]['W'].T\n",
    "                #print 'dA[{}] = W[{}].T dZ[{}] : '.format(l+1, l+1, l+1),\n",
    "                #print '({}, {})({}, {})'.format(Wlt.shape[0], Wlt.shape[1], dZl.shape[0], dZl.shape[1])\n",
    "                dAl = np.dot(Wlt, dZl)\n",
    "                gZl = self.cache[l]['dg']\n",
    "                #print \"dZ[{}] = dA[{}] * g'(dZ[{}]) : \".format(l, l+1, l),\n",
    "                #print '({}, {})({}, {})'.format(dAl.shape[0], dAl.shape[1], gZl.shape[0], gZl.shape[1])\n",
    "                dZl = dAl * gZl\n",
    "            if l == 1:\n",
    "                dWl = 1./X.shape[1] * np.dot(dZl, self.X.T)\n",
    "            else:\n",
    "                dWl = 1./X.shape[1] * np.dot(dZl, self.cache[l-1]['A'].T)\n",
    "            dbl = 1./X.shape[1] * np.sum(dZl, axis=1, keepdims=True)\n",
    "            self.dv[l] = {'dW': dWl, 'db': dbl}\n",
    "            \n",
    "    def update(self, lr):\n",
    "     dZ^{[3]} = -\\frac{Y}{\\hat{Y}} + \\frac{1-Y}{1-\\hat{Y}} \\\\dW^{[3]} = \\frac{1}{m} dZ^{[3]}A^{[2]}^T \\\\ \n",
    "   for l in sorted(self.nn):\n",
    "            self.nn[l]['W'] -= lr * self.dv[l]['dW'] \n",
    "            self.nn[l]['b'] -= lr * self.dv[l]['db']\n",
    "        \n",
    "    def summary(self):\n",
    "        total = 0\n",
    "        print 'Layer name\\tShape\\t\\tParam #'\n",
    "        print '========================================'\n",
    "        print 'Input:\\t\\t({}, {})'.format(self.X.shape[0], self.X.shape[1])\n",
    "        print '----------------------------------------'\n",
    "        for l in sorted(self.nn):\n",
    "            W = self.nn[l]['W']\n",
    "            b = self.nn[l]['b']\n",
    "            params_W = W.shape[0] * W.shape[1]\n",
    "            params_b = b.shape[0] * b.shape[1]\n",
    "            total += params_W + params_b\n",
    "            print 'W{}\\t\\t({}, {})\\t\\t{}'.format(l, W.shape[0], W.shape[1], params_W)\n",
    "            print 'b{}\\t\\t({}, {})\\t\\t{}'.format(l, b.shape[0], b.shape[1], params_b)\n",
    "            if l != len(self.nn):\n",
    "                print '----------------------------------------'\n",
    "        print '========================================'\n",
    "        print 'Total params: {}'.format(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name\tShape\t\tParam #\n",
      "========================================\n",
      "Input:\t\t(3, 5)\n",
      "----------------------------------------\n",
      "W1\t\t(3, 3)\t\t9\n",
      "b1\t\t(3, 1)\t\t3\n",
      "----------------------------------------\n",
      "W2\t\t(4, 3)\t\t12\n",
      "b2\t\t(4, 1)\t\t4\n",
      "----------------------------------------\n",
      "W3\t\t(1, 4)\t\t4\n",
      "b3\t\t(1, 1)\t\t1\n",
      "========================================\n",
      "Total params: 33\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 10\n",
    "dims = [3, 4, 1]\n",
    "\n",
    "# 5 examples, 3 features\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.1, 0.3, 0.09],\n",
    "    [0.1, 0.2, 0.1, 0.3, 0.09],\n",
    "    [0.1, 0.2, 0.1, 0.3, 0.09]\n",
    "])\n",
    "Y = np.array([1, 2, 1, 3, 4]).reshape(1,5)\n",
    "\n",
    "nn = NeuralNetwork(X, Y, dims)\n",
    "nn.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 :: Loss: 8.19672592762\n",
      "Epoch: 1 :: Loss: 8.19672592762\n",
      "Epoch: 2 :: Loss: 8.18898000801\n",
      "Epoch: 3 :: Loss: 8.18126601948\n",
      "Epoch: 4 :: Loss: 8.17358370352\n",
      "Epoch: 5 :: Loss: 8.16593280471\n",
      "Epoch: 6 :: Loss: 8.15831307073\n",
      "Epoch: 7 :: Loss: 8.15072425229\n",
      "Epoch: 8 :: Loss: 8.14316610308\n",
      "Epoch: 9 :: Loss: 8.1356383797\n",
      "Epoch: 10 :: Loss: 8.12814084166\n"
     ]
    }
   ],
   "source": [
    "nn.forward()\n",
    "print 'Epoch: 0 :: Loss: {}'.format(nn.loss)\n",
    "for i in range(1, nb_epochs+1):\n",
    "    nn.forward()\n",
    "    nn.backward()\n",
    "    nn.update(0.000001)\n",
    "    print 'Epoch: {} :: Loss: {}'.format(i, nn.loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
