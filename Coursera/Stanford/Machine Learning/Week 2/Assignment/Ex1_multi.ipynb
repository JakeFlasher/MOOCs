{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Linear regression with multiple variables\n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file ``ex1data2.txt`` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.\n",
    "\n",
    "The ``ex1_multi.m`` script has been set up to help you step through this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Load Data\n",
    "data = load('/home/roger/Workspace/MOOC/Coursera/MLearning/Week 2/Assignment/ex1/ex1data2.txt');\n",
    "X = data(:, 1:2);\n",
    "y = data(:, 3);\n",
    "m = length(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Normalization\n",
    "\n",
    "The ``ex1_multi.m`` script will start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n",
    "\n",
    "Your task here is to complete the code in ``featureNormalize.m`` to\n",
    "  * Subtract the mean value of each feature from the dataset.\n",
    "  * After subtracting the mean, additionally scale (divide) the feature values by their respective \"standard deviations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function [X_norm, mu, sigma] = featureNormalize(X)\n",
    "    % Instructions: First, for each feature dimension, compute the mean\n",
    "    %               of the feature and subtract it from the dataset,\n",
    "    %               storing the mean value in mu. Next, compute the \n",
    "    %               standard deviation of each feature and divide\n",
    "    %               each feature by it's standard deviation, storing\n",
    "    %               the standard deviation in sigma. \n",
    "    %\n",
    "    %               Note that X is a matrix where each column is a \n",
    "    %               feature and each row is an example. You need \n",
    "    %               to perform the normalization separately for \n",
    "    %               each feature. \n",
    "    %\n",
    "    % Hint: You might find the 'mean' and 'std' functions useful.\n",
    "    X_norm = X;\n",
    "    mu = mean(X);\n",
    "    sigma = std(X);\n",
    "\n",
    "    for i = 1:size(X, 2)\n",
    "        X_norm(:,i) = X(:,i) - mu(i);\n",
    "        X_norm(:,i) = X_norm(:,i) / sigma(i);\n",
    "    end\n",
    "end\n",
    "[X mu sigma] = featureNormalize(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gradient Descent\n",
    "\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged.\n",
    "\n",
    "You should complete the code in ``computeCostMulti.m`` and ``gradientDescentMulti.m`` to implement the cost function and gradient descent for linear regression with multiple variables. If your code in the previous part (single variable) already supports multiple variables, you can use it here too.\n",
    "\n",
    "Make sure your code supports any number of features and is well-vectorized. You can use 'size(X, 2)' to find out how many features are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =    6.5592e+10\r\n"
     ]
    }
   ],
   "source": [
    "function J = computeCostMulti(X, y, theta)\n",
    "    %COMPUTECOSTMULTI Compute cost for linear regression with multiple variables\n",
    "    %   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the\n",
    "    %   parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly \n",
    "    J = 0;\n",
    "\n",
    "    m = size(X, 1);         % number of training examples\n",
    "    predictions = X*theta;  % predictions of hypothesis on all m examples\n",
    "    sqrErrors = (predictions-y).^2; %squared errors\n",
    "\n",
    "    J = 1/(2*m) * sum(sqrErrors);\n",
    "end\n",
    "\n",
    "computeCostMulti(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation Note: In the multivariate case, the cost function can also be written in the following vectorized form:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)$$\n",
    "\n",
    "The vectorized version is efficient when youâ€™re working with numerical computing tools like Octave/MATLAB. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =    6.5592e+10\r\n"
     ]
    }
   ],
   "source": [
    "function J = computeCostMulti(X, y, theta)\n",
    "    %COMPUTECOSTMULTI Compute cost for linear regression with multiple variables\n",
    "    %   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the\n",
    "    %   parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly \n",
    "    J = 0;\n",
    "\n",
    "    m = size(X, 1);                                     % number of training examples\n",
    "    sqrErrors = ((X * theta) - y)' * ((X * theta) - y); %squared errors\n",
    "    J = 1/(2*m) * sqrErrors;\n",
    "end\n",
    "\n",
    "computeCostMulti(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "    %GRADIENTDESCENTMULTI Performs gradient descent to learn theta\n",
    "    %   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by\n",
    "    %   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "    J_history = zeros(num_iters, 1);\n",
    "\n",
    "    for iter = 1:num_iters\n",
    "        predictions = X * theta;       % predictions of hypothesis on all m examples\n",
    "        errors = (predictions - y);    %errors\n",
    "        delta = 1/(m) * X' * errors;\n",
    "        theta = theta - alpha * delta;\n",
    "\n",
    "        % Save the cost J in every iteration    \n",
    "        J_history(iter) = computeCostMulti(X, y, theta);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Normal Equations\n",
    "\n",
    "In the lecture videos, you learned that the closed-form solution to linear regression is:\n",
    "\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no ''loop until convergence'' like in gradient descent. Complete the code in `normalEqn.m` to use the formula above to calculate $\\theta$. Remember that while you don't need to scale your features, we still\n",
    "need to add a column of 1's to the X matrix to have an intercept term ($\\theta_0$).\n",
    "\n",
    "The code in `ex1.m` will add the column of 1's to X for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =\n",
      "\n",
      "   1.4086e+02\n",
      "   1.6978e+04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function [theta] = normalEqn(X, y)\n",
    "    %NORMALEQN Computes the closed-form solution to linear regression \n",
    "    %   NORMALEQN(X,y) computes the closed-form solution to linear \n",
    "    %   regression using the normal equations.\n",
    "    theta = zeros(size(X, 2), 1);\n",
    "    theta = pinv(X'*X)*X'*y;\n",
    "end\n",
    "\n",
    "theta = normalEqn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.15.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
