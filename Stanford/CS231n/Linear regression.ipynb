{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manhattan distance\n",
    "\n",
    "More formally, we can define the Manhattan distance, also known as the L1-distance, between two points in an Euclidean space with fixed Cartesian coordinate system is defined as the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. For example, in the plane, the Manhattan distance between the point $P1$ with coordinates $(x1, y1)$ and the point $P2$ at $(x2, y2)$ is $\\left|x_1 - x_2\\right| + \\left|y_1 - y_2\\right|$ and can be defined as:\n",
    "\n",
    "$$d1(I1,I2) = \\sum_p |I_p^1 - I_p^2|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "        self.Xtr = X\n",
    "        self.ytr = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)\n",
    "\n",
    "        for i in xrange(num_test):\n",
    "            # find the nearest training image to the i'th test image\n",
    "            # using the L1 distance\n",
    "            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)\n",
    "            min_index = np.argmin(distances) # get index with the smallest distance\n",
    "            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Support Vector Machine loss\n",
    "\n",
    "Loss function measures our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we're doing a poor job of classifying the training data, and it will be low if we're doing well.\n",
    "\n",
    "The SVM loss is set up so that the SVM \"wants\" the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\\Delta$. Notice that it's sometimes helpful to anthropomorphise the loss functions as we did above: The SVM \"wants\" a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "Let's now get more precise. Recall that for the i-th example we are given the pixels of image $x_i$ and the label $y_i$ that specifies the index of the correct class. The score function takes the pixels and computes the vector $f(x_i,W)$ of class scores, which we will abbreviate to $s$ (short for scores). For example, the score for the j-th class is the j-th element: $s_j=f(x_i,W)_j$. The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + \\Delta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5\n"
     ]
    }
   ],
   "source": [
    "# implementation of the loss function\n",
    "import numpy as np\n",
    "\n",
    "def L_i_vectorized(x, y, W, delta=1):\n",
    "    scores = np.array([[3.2, 1.3, 2.2],[5.1, 4.9, 2.5],[-1.7, 2.0, -3.1]])# W.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + delta)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "\n",
    "W = np.array([[0.2, -0.5, 0.1, 2.0],[1.5, 1.3, 2.1, 0.0],[0.0, 0.25, 0.2, -0.3]])\n",
    "x = np.array([56, 231, 24, 2])\n",
    "b = np.array([1.1, 3.2, -1.2])\n",
    "\n",
    "#Wx = np.array([[3.2, 1.3, 2.2],[5.1, 4.9, 2.5],[-1.7, 2.0, -3.1]])\n",
    "delta = 1\n",
    "y = 0\n",
    "print L_i_vectorized(x, y, W, delta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in xrange(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "  \"\"\"\n",
    "  fully-vectorized implementation :\n",
    "  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "  - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "  - W are weights (e.g. 10 x 3073)\n",
    "  \"\"\"\n",
    "  # evaluate loss over all examples in X without using any for loops\n",
    "  # left as exercise to reader in the assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
