{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Katie explained in a video a problem that arose in preparing Chris and Sara's email for the author identification project; \n",
    "it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair \n",
    "advantage to an algorithm). You'll work through that discovery process here.\n",
    "\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project.\n",
    "A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree \n",
    "is to use a small training set and lots of features. \n",
    "If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low? - Low\n",
    "\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code \n",
    "in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. How\n",
    "many training points are there, according to the starter code?\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "find_signature.py\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"your_word_data.pkl\" \n",
    "authors_file = \"your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, \n",
    "                                                                                             test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree Accuracy: 0.947667804323\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What's the accuracy of the decision tree you just made? (Remember, we're setting up our decision tree to overfit -- ideally, we \n",
    "want to see the test accuracy as relatively low.)\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf =  DecisionTreeClassifier() #min_samples_split=40)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'DTree Accuracy:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 33614 has an importance of: 0.764706\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take your (overfit) decision tree and use the feature_importances_ attribute to get a list of the relative importance of all \n",
    "the features being used. We suggest iterating through this list (it's long, since this is text data) and only printing out the \n",
    "feature importance if it's above some threshold (say, 0.2--remember, if all words were equally important, each one would give \n",
    "an importance of far less than 0.01). What's the importance of the most important feature? What is the number of this feature?\n",
    "\"\"\"\n",
    "labels = clf.classes_\n",
    "featimp = clf.feature_importances_\n",
    "maxfeat = 0\n",
    "for k, feat in enumerate(featimp):\n",
    "    if feat > 0.2:\n",
    "        maxfeat = k\n",
    "        print 'The feature %d has an importance of: %f' % (k, feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word with the highest importance is: sshacklensf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you \n",
    "obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the \n",
    "TfIdf by calling get_feature_names() on it; pull out the word that's causing most of the discrimination of the decision tree. \n",
    "What is it? Does it make sense as a word that's uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?\n",
    "\"\"\"\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "print 'The word with the highest importance is: %s' % feat_names[maxfeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This word seems like an outlier in a certain sense, so let's remove it and refit. Go back to text_learning/vectorize_text.py, and \n",
    "remove this word from the emails using the same method you used to remove \"sara\", \"chris\", etc. Rerun vectorize_text.py, and \n",
    "once that finishes, rerun find_signature.py.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "parse_out_email_text.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "    \"\"\"\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = ''\n",
    "        for word in text_string.strip().split():\n",
    "            stem = stemmer.stem(word)\n",
    "            words += stem+' '\n",
    "    return words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vectorize_text.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../ud120-projects/\" )\n",
    "#from parse_out_email_text import parseOutText\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        path = os.path.join('../ud120-projects/', path[:-1])\n",
    "        #print path\n",
    "        email = open(path, \"r\")\n",
    "\n",
    "        ### use parseOutText to extract the text from the opened email\n",
    "        text = parseOutText(email)\n",
    "\n",
    "        ### use str.replace() to remove any instances of the words\n",
    "        users = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\", \"cgermannsf\"]\n",
    "        for user in users:\n",
    "            text = text.replace(user, '')\n",
    "\n",
    "        ### append the text to word_data\n",
    "        word_data.append(text)\n",
    "\n",
    "        ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "        if name == 'sara':\n",
    "            from_data.append(0)\n",
    "        elif name == 'chris':\n",
    "            from_data.append(1)\n",
    "\n",
    "        email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree Accuracy: 0.816837315131\n",
      "The feature 21323 has an importance of: 0.363636\n",
      "The word with the highest importance is: houectect\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Rerun vectorize_text.py, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like \n",
    "a signature-type word? (Define an outlier as a feature with importance > 0.2, as before).\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "find_signature.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"your_word_data.pkl\" \n",
    "authors_file = \"your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, \n",
    "                                                                                             test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf =  DecisionTreeClassifier() #min_samples_split=40)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'DTree Accuracy:', accuracy\n",
    "\n",
    "labels = clf.classes_\n",
    "featimp = clf.feature_importances_\n",
    "maxfeat = 0\n",
    "for k, feat in enumerate(featimp):\n",
    "    if feat > 0.2:\n",
    "        maxfeat = k\n",
    "        print 'The feature %d has an importance of: %f' % (k, feat)\n",
    "        \n",
    "feat_names = vectorizer.get_feature_names()\n",
    "print 'The word with the highest importance is: %s' % feat_names[maxfeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
