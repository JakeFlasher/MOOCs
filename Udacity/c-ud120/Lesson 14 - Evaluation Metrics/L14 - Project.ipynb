{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.724137931034\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Go back to your code from the last lesson, where you built a simple first iteration of a POI identifier using a decision tree \n",
    "and one feature. Copy the POI identifier that you built into the skeleton code in evaluation/evaluate_poi_identifier.py. \n",
    "Recall that at the end of that project, your identifier had an accuracy (on the test set) of 0.724. Not too bad, right? Let's dig\n",
    "into your predictions a little more carefully.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "feature_format.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\"\"\"\n",
    "validate_poi.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "    Starter code for the validation mini-project.\n",
    "    The first step toward building your POI identifier!\n",
    "\n",
    "    Start by loading/formatting the data\n",
    "\n",
    "    After that, it's not our code anymore--it's yours!\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn import cross_validation\n",
    "\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, \n",
    "                                                                                             test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf =  tree.DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'Accuracy:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predicted POIs: 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "How many POIs are predicted for the test set for your POI identifier?\n",
    "\n",
    "(Note that we said test set! We are not looking for the number of POIs in the whole dataset.)\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "pois_in_test = Counter(pred)\n",
    "print 'Total predicted POIs: %d' % pois_in_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total people in test set: 29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "How many people total are in your test set?\n",
    "\"\"\"\n",
    "print 'Total people in test set: %d' % len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862068965517\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?\n",
    "\"\"\"\n",
    "all_zeros = [0.0]*(len(pred))\n",
    "accuracy = accuracy_score(all_zeros, labels_test)\n",
    "print 'Accuracy:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some \n",
    "special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, \n",
    "and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in \n",
    "sklearn.metrics to compute those quantities.\n",
    "\n",
    "What's the precision?\n",
    "\"\"\"\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "p = precision_score(labels_test, all_zeros)\n",
    "print 'Precision: ', p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What's the recall? \n",
    "\n",
    "(Note: you may see a message like UserWarning: The precision and recall are equal to zero for some labels. Just like the message \n",
    "says, there can be problems in computing other metrics (like the F1 score) when precision and/or recall are zero, and it wants \n",
    "to warn you when that happens.) \n",
    "\n",
    "Obviously this isn't a very optimized machine learning strategy (we haven't tried any algorithms besides the decision tree, or \n",
    "tuned any parameters, or done any feature selection), and now seeing the precision and recall should make that much more apparent\n",
    "than the accuracy did.\n",
    "\"\"\"\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "r = recall_score(labels_test, all_zeros)\n",
    "print 'Recall: ', r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.666666666667\n",
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the final project you'll work on optimizing your POI identifier, using many of the tools learned in this course. Hopefully one\n",
    "result will be that your precision and/or recall will go up, but then you'll have to be able to interpret them. \n",
    "\n",
    "Here are some made-up predictions and true labels for a hypothetical test set; fill in the following boxes to practice \n",
    "identifying true positives, false positives, true negatives, and false negatives. Let's use the convention that \"1\" signifies a \n",
    "positive result, and \"0\" a negative. (this is fabricated data, just to give you some practice)\n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "How many true positives are there? 6\n",
    "\n",
    "How many true negatives are there in this example? 9\n",
    "\n",
    "How many false positives are there? 3\n",
    "\n",
    "How many false negatives are there? 2\n",
    "\n",
    "What's the precision of this classifier?\n",
    "\n",
    "P = true_postives / (true_positives + false_positives)\n",
    "P = 6/(6+3)\n",
    "P = 0.6666666\n",
    "\n",
    "What's the recall of this classifier?\n",
    "\n",
    "R = true_positives / (true_positives + false_negatives)\n",
    "R = 6/(6+2)\n",
    "R = 0.75\n",
    "\n",
    "Fill in the blank:\n",
    "\n",
    "\"My true positive rate is high, which means that when a _POI_ is present in the test data, I am good at flagging him or her.\"\n",
    "\"My identifier doesn't have great _PRECISION_, but it does have good _RECALL_. That means that, nearly every time a POI shows \n",
    "up in my test set, I am able to identify him or her. The cost of this is that I sometimes get some false positives, where \n",
    "non-POIs get flagged.\"\n",
    "\n",
    "\"My identifier doesn't have great _RECALL_, but it does have good _PRECISION_. That means that whenever a POI gets flagged in my \n",
    "test set, I know with a lot of confidence that it's very likely to be a real POI and not a false alarm. On the other hand, the \n",
    "price I pay for this is that I sometimes miss real POIs, since I'm effectively reluctant to pull the trigger on edge cases.\"\n",
    "\n",
    "\"My identifier has a really great _F1-SCORE_. This is the best of both worlds. Both my false positive and false negative rates \n",
    "are _LOW_, which means that I can identify POI's reliably and accurately. If my identifier finds a POI then the person is almost \n",
    "certainly a POI, and if the identifier does not flag someone, then they are almost certainly not a POI\"\n",
    "\"\"\"\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "p = precision_score(true_labels, predictions)\n",
    "print 'Precision: ', p\n",
    "\n",
    "r = recall_score(true_labels, predictions)\n",
    "print 'Recall: ', r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
