{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this mini-project, we'll tackle the exact same email author ID problem as the Naive Bayes mini-project, but now with an SVM. \n",
    "What we find will help clarify some of the practical differences between the two algorithms. This project also gives us a chance \n",
    "to play around with parameters a lot more than Naive Bayes did, so we will do that too.\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"word_data.pkl\", authors_file=\"email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.984072810011\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Go to the svm directory to find the starter code (svm/svm_author_id.py).\n",
    "\n",
    "Import, create, train and make predictions with the sklearn SVC classifier. When creating the classifier, use a linear kernel \n",
    "(if you forget this step, you will be unpleasantly surprised by how long the classifier takes to train). What is the accuracy \n",
    "of the classifier?\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC(kernel='linear', gamma='auto', C=1.0)\n",
    "\n",
    "### train step\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### use the trained classifier to predict labels for the test features\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "### calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'SVM Accuracy:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 181.191 s\n",
      "predicting time: 18.813 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Place timing code around the fit and predict functions, like you did in the Naive Bayes mini-project. How do the training and \n",
    "prediction times compare to Naive Bayes?\n",
    "\"\"\"\n",
    "\n",
    "clf = svm.SVC(kernel='linear', gamma='auto', C=1.0)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "t0 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print \"predicting time:\", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.884527872582\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "One way to speed up an algorithm is to train it on a smaller training dataset. The tradeoff is that the accuracy almost always \n",
    "goes down when you do this. Let's explore this more concretely: add in the following two lines immediately before training your \n",
    "classifier. \n",
    "\n",
    "features_train = features_train[:len(features_train)/100] \n",
    "labels_train = labels_train[:len(labels_train)/100] \n",
    "\n",
    "These lines effectively slice the training dataset down to 1% of its original size, tossing out 99% of the training data. You \n",
    "can leave all other code unchanged. What's the accuracy now?\n",
    "\"\"\"\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "clf = svm.SVC(kernel='linear', gamma='auto', C=1.0)\n",
    "\n",
    "### train step\n",
    "features_train = features_train[:len(features_train)/100] \n",
    "labels_train = labels_train[:len(labels_train)/100] \n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### use the trained classifier to predict labels for the test features\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "### calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'SVM Accuracy:', accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "SVM Accuracy: 0.616040955631\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keep the training set slice code from the last quiz, so that you are still training on only 1% of the full training set. Change \n",
    "the kernel of your SVM to \"rbf\". What's the accuracy now, with this more complex kernel?\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "features_train = features_train[:len(features_train)/100] \n",
    "labels_train = labels_train[:len(labels_train)/100] \n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=1.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### use the trained classifier to predict labels for the test features\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "### calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'SVM Accuracy:', accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy using C=10.0: 0.616040955631\n",
      "SVM Accuracy using C=100.0: 0.616040955631\n",
      "SVM Accuracy using C=1000.0: 0.821387940842\n",
      "SVM Accuracy using C=10000.0: 0.892491467577\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keep the training set size and rbf kernel from the last quiz, but try several values of C (say, 10.0, 100., 1000., and 10000.). \n",
    "Which one gives the best accuracy?\n",
    "\"\"\"\n",
    "# C = 10.0\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=10.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'SVM Accuracy using C=10.0:', accuracy\n",
    "\n",
    "# C = 100.0\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=100.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'SVM Accuracy using C=100.0:', accuracy\n",
    "\n",
    "# C = 1000.0\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=1000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'SVM Accuracy using C=1000.0:', accuracy\n",
    "\n",
    "# C = 10000.0\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=10000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'SVM Accuracy using C=10000.0:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "SVM Accuracy using full dataset and C=10000.0: 0.990898748578\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that you've optimized C for the RBF kernel, go back to using the full training set. In general, having a larger training set \n",
    "will improve the performance of your algorithm, so (by tuning C and training on a large dataset) we should get a fairly optimized \n",
    "result. What is the accuracy of the optimized SVM?\n",
    "\"\"\"\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=10000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### use the trained classifier to predict labels for the test features\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "### calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "\n",
    "print 'SVM Accuracy using full dataset and C=10000.0:', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM answers for (10, 26, 50) elements are:  (1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What class does your SVM (0 or 1, corresponding to Sara and Chris respectively) predict for element 10 of the test set? The 26th? \n",
    "The 50th? (Use the RBF kernel, C=10000, and 1% of the training set. Normally you'd get the best results using the full training \n",
    "set, but we found that using 1% sped up the computation considerably and did not change our results--so feel free to use that \n",
    "shortcut here.)\n",
    "\n",
    "And just to be clear, the data point numbers that we give here (10, 26, 50) assume a zero-indexed list. So the correct answer \n",
    "for element #100 would be found using something like answer=predictions[100]\n",
    "\"\"\"\n",
    "features_train = features_train[:len(features_train)/100] \n",
    "labels_train = labels_train[:len(labels_train)/100] \n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=10000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "answer= (pred[10], pred[26], pred[50]) \n",
    "print 'SVM answers for (10, 26, 50) elements are: ', answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "There are 877 elements predicted as Chris\n",
      "Total of 1758 elements\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "There are over 1700 test events--how many are predicted to be in the \"Chris\" (1) class? (Use the RBF kernel, C=10000., \n",
    "and the full training set.)\n",
    "\"\"\"\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto', C=10000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "chris = 0\n",
    "for n, el in enumerate(pred, 1):\n",
    "    if el == 1:\n",
    "        chris += 1\n",
    "print 'There are %d elements predicted as Chris' % chris\n",
    "print 'Total of %d elements' % n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
