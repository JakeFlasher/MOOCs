{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this project, you will use regression to predict financial data for Enron employees and associates. Once you know some \n",
    "financial data about an employee, like their salary, what would you predict for the size of their bonus?\n",
    "\n",
    "Run the starter code found in regression/finance_regression.py. This will draw a scatterplot, with all the data points drawn in. \n",
    "What target are you trying to predict? What is the input feature being used to predict it?\n",
    "\n",
    "Mentally (or better yet, print out the scatterplot and use paper and pencil) sketch out the regression line that you roughly \n",
    "predict.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "feature_format.py\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "finance_regression.py\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the regression mini-project.\n",
    "    \n",
    "    Loads up/formats a modified version of the dataset\n",
    "    (why modified?  we've removed some trouble points\n",
    "    that you'll find yourself in the outliers mini-project).\n",
    "\n",
    "    Draws a little scatterplot of the training/testing data\n",
    "\n",
    "    You fill in the regression code where indicated:\n",
    "\"\"\"    \n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "dictionary = pickle.load( open(\"final_project_dataset_modified.pkl\", \"r\") )\n",
    "\n",
    "### list the features you want to look at--first item in the \n",
    "### list will be the \"target\" feature\n",
    "features_list = [\"bonus\", \"salary\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "### Your regression goes here!\n",
    "### Please name it reg, so that the plotting code below picks it up and \n",
    "### plots it correctly. Don't forget to change the test_color above from \"b\" to\n",
    "### \"r\" to differentiate training points from test points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color ) \n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color ) \n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:  5.44814028881\n",
      "Intercept:  -102360.543294\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In regression, you need training and testing data, just like in classification. This has already been set up in the starter code. \n",
    "Change the value of test_color from \"b\" to \"r\" (for \"red\"), and rerun. Note: For those students converting Python 2 code to \n",
    "Python 3, see below for some important remarks regarding compatibility.\n",
    "\n",
    "You will be fitting your regression using the blue (training) points only. (You may have noticed that instead of the standard \n",
    "10%, we've put 50% of our data into the test set--that's because in Part 5, we will switch the training and testing datasets \n",
    "around and splitting the data evenly makes this more straightforward.)\n",
    "\n",
    "Import LinearRegression from sklearn, and create/fit your regression. Name it reg so that the plotting code will show it \n",
    "overlaid on the scatterplot. Does it fall approximately where you expected it?\n",
    "\n",
    "Extract the slope (stored in the reg.coef_ attribute) and the intercept. What are the slope and intercept?\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print 'Slope: ', reg.coef_[0]\n",
    "print 'Intercept: ', reg.intercept_\n",
    "\n",
    "#print 'Net worth prediction: ', reg.predict([[27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### stats on train dataset #####\n",
      "r-squared score:  0.0455091926995\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imagine you were a less savvy machine learner, and didn't know to test on a holdout test set. Instead, you tested on the \n",
    "same data that you used to train, by comparing the regression predictions to the target values (i.e. bonuses) in the training \n",
    "data. What score do you find? You may not have an intuition yet for what a \"good\" score is; this score isn't very good (but it \n",
    "could be a lot worse).\n",
    "\"\"\"\n",
    "print '###### stats on train dataset #####'\n",
    "print 'r-squared score: ', reg.score(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### stats on test dataset #####\n",
      "r-squared score:  -1.48499241737\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now compute the score for your regression on the test data, like you know you should. What's that score on the testing data? \n",
    "If you made the mistake of only assessing on the training data, would you overestimate or underestimate the performance of \n",
    "your regression?\n",
    "\"\"\"\n",
    "print '###### stats on test dataset #####'\n",
    "print 'r-squared score: ', reg.score(feature_test, target_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:  1.19214698985\n",
      "Intercept:  554478.756215\n",
      "r-squared score train:  0.217085971258\n",
      "r-squared score test:  -0.59271289995\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "There are lots of finance features available, some of which might be more powerful than others in terms of predicting a \n",
    "person's bonus. For example, suppose you thought about the data a bit and guess that the \"long_term_incentive\" feature, which \n",
    "is supposed to reward employees for contributing to the long-term health of the company, might be more closely related to a \n",
    "person's bonus than their salary is.\n",
    "\n",
    "A way to confirm that you're right in this hypothesis is to regress the bonus against the long term incentive, and see if the \n",
    "regression score is significantly higher than regressing the bonus against the salary. Perform the regression of bonus against \n",
    "long term incentive -- what's the score on the test data?\n",
    "\"\"\"\n",
    "features_list = [\"bonus\", \"long_term_incentive\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print 'Slope: ', reg.coef_[0]\n",
    "print 'Intercept: ', reg.intercept_\n",
    "print 'r-squared score train: ', reg.score(feature_train, target_train)\n",
    "print 'r-squared score test: ', reg.score(feature_test, target_test)\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color ) \n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color ) \n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:  2.27410114127\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a sneak peek of the next lesson, on outlier identification and removal. Go back to a setup where you are using the salary \n",
    "to predict the bonus, and rerun the code to remind yourself of what the data look like. You might notice a few data points that \n",
    "fall outside the main trend, someone who gets a high salary (over a million dollars!) but a relatively small bonus. This is an \n",
    "example of an outlier, and we'll spend lots of time on them in the next lesson.\n",
    "\n",
    "A point like this can have a big effect on a regression: if it falls in the training set, it can have a significant effect on \n",
    "the slope/intercept if it falls in the test set, it can make the score much lower than it would otherwise be. As things stand \n",
    "right now, this point falls into the test set (and probably hurting the score on our test data as a result). Let's add a little \n",
    "hack to see what happens if it falls in the training set instead. Add these two lines near the bottom of finance_regression.py, \n",
    "right before plt.xlabel(features_list[1]):\n",
    "\n",
    "reg.fit(feature_test, target_test)\n",
    "plt.plot(feature_train, reg.predict(feature_train), color=\"b\") \n",
    "\n",
    "Now we'll be drawing two regression lines, one fit on the test data (with outlier) and one fit on the training data (no outlier). \n",
    "Look at the plot now--big difference, huh? That single outlier is driving most of the difference. What's the slope of the new \n",
    "regression line?\n",
    "\n",
    "(That's a big difference, and it's mostly driven by the outliers. The next lesson will dig into outliers in more detail so you \n",
    "have tools to detect and deal with them.)\n",
    "\"\"\"\n",
    "features_list = [\"bonus\", \"salary\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "### Your regression goes here!\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color ) \n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color ) \n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "reg.fit(feature_test, target_test)\n",
    "plt.plot(feature_train, reg.predict(feature_train), color=\"g\") \n",
    "\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print 'Slope: ', reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
