{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can play with k-means clustering yourself here: http://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\"\"\"\n",
    "In this project, we'll apply k-means clustering to our Enron financial data. Our final goal, of course, is to identify persons \n",
    "of interest; since we have labeled data, this is not a question that particularly calls for an unsupervised approach like \n",
    "k-means clustering.\n",
    "\n",
    "Nonetheless, you'll get some hands-on practice with k-means in this project, and play around with feature scaling, which will \n",
    "give you a sneak preview of the next lesson's material.\n",
    "\n",
    "The starter code can be found in k_means/k_means_cluster.py, which reads in the email + financial (E+F) dataset and gets us \n",
    "ready for clustering. You'll start with performing k-means based on just two financial features--take a look at the code, and \n",
    "determine which features the code uses for clustering.\n",
    "\n",
    "Run the code, which will create a scatterplot of the data. Think a little bit about what clusters you would expect to arise if \n",
    "2 clusters are created.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "feature_format.py\n",
    "\"\"\" \n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions object named pred found, no clusters to plot\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "k_means_clustering.py\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python \n",
    "\n",
    "\"\"\" \n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"final_project_dataset.pkl\", \"r\") )\n",
    "### there's an outlier--remove it! \n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "\n",
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2 in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster using K-means\n",
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "clu = KMeans(n_clusters=2)\n",
    "pred = clu.fit_predict(data)\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"2clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add a third feature to features_list, \"total_payments\". Now rerun clustering, using 3 input features instead of 2 (obviously we \n",
    "can still only visualize the original 2 dimensions). Compare the plot with the clusterings to the one you obtained with 2 input \n",
    "features. Do any points switch clusters? How many? This new clustering, using 3 features, couldn't have been guessed by eye--it \n",
    "was the k-means algorithm that identified it.\n",
    "\n",
    "(You'll need to change the code that makes the scatterplot to accommodate 3 features instead of 2, see the comments in the \n",
    "starter code for instructions on how to do this.)\n",
    "\"\"\"\n",
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "feature_3 = \"total_payments\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2, feature_3]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2, _ in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "from sklearn.cluster import KMeans\n",
    "clu = KMeans(n_clusters=2)\n",
    "pred = clu.fit_predict(data)\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"3clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum \"exercised_stock_options\":  3285\n",
      "Maximum \"exercised_stock_options\":  34348384\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the next lesson, we'll talk about feature scaling. It's a type of feature preprocessing that you should perform before some \n",
    "classification and regression tasks. Here's a sneak preview that should call your attention to the general outline of what \n",
    "feature scaling does.\n",
    "\n",
    "What are the maximum and minimum values taken by the \"exercised_stock_options\" feature used in this example?\n",
    "\n",
    "(NB: if you look at finance_features, there are some \"NaN\" values that have been cleaned away and replaced with zeroes--so while \n",
    "those might look like the minima, it's a bit deceptive because they're more like points for which we don't have information, and \n",
    "just have to put in a number. So for this question, go back to data_dict and look for the maximum and minimum numbers that show \n",
    "up there, ignoring all the \"NaN\" entries.)\n",
    "\"\"\"\n",
    "\n",
    "def get_min_max(feature):\n",
    "    min_feat = 'NaN'\n",
    "    max_feat = 'NaN'\n",
    "    for name in data_dict:\n",
    "        feat = data_dict[name][feature]\n",
    "        if feat != 'NaN':\n",
    "            if min_feat == 'NaN':\n",
    "                min_feat = feat\n",
    "                max_feat = feat\n",
    "            if feat < min_feat:\n",
    "                min_feat = feat\n",
    "            if feat > max_feat:\n",
    "                max_feat = feat\n",
    "    return min_feat, max_feat\n",
    "\n",
    "min_stock, max_stock = get_min_max(\"exercised_stock_options\")    \n",
    "print 'Minimum \"exercised_stock_options\": ', min_stock\n",
    "print 'Maximum \"exercised_stock_options\": ', max_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum \"exercised_stock_options\":  477\n",
      "Maximum \"exercised_stock_options\":  1111258\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What are the maximum and minimum values taken by \"salary\"?\n",
    "\n",
    "(NB: same caveat as in the last quiz. If you look at finance_features, there are some \"NaN\" values that have been cleaned away \n",
    "and replaced with zeroes--so while those might look like the minima, it's a bit deceptive because they're more like points for \n",
    "which we don't have information, and just have to put in a number. So for this question, go back to data_dict and look for the \n",
    "maximum and minimum numbers that show up there, ignoring all the \"NaN\" entries.)\n",
    "\"\"\"\n",
    "min_salary, max_salary = get_min_max(\"salary\")    \n",
    "print 'Minimum \"exercised_stock_options\": ', min_salary\n",
    "print 'Maximum \"exercised_stock_options\": ', max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The plot on the next slide shows the exact same clustering code that you just wrote, but in this example we applied feature \n",
    "scaling before performing the clustering.\n",
    "\n",
    "We want you to compare the clustering with scaling (on the next slide) with the first clustering visualization you produced, \n",
    "when you used two features in your clustering algorithm.\n",
    "\n",
    "Notice that now the range of the features has changed to [0.0, 1.0]. That's the only change we've made.\n",
    "\n",
    "In the next lesson you'll learn a lot more about what feature scaling means, but for now, just look at the effect on the \n",
    "clusters--which point(s) switch their associated cluster?\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
