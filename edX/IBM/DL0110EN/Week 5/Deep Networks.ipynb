{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a popular regularization technique for neural networks. Dropout is a probability that tells us how likely we are to turn off an individual neuron independently. Let's say that we have a probability of 0.2 to turn off a neuron. So, what we're doing is actually calculating the value for the neuron in the forward step. We repeat the process again for the $k$-th iteration. Important to say that PyTorch normalizes the values ($\\frac{1}{1-p}$ in each layer) in the training phase - other methods may do it in the testing phase.\n",
    "\n",
    "<img src=\"images/dropout.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, n_hidden, out_size, p=0):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nn.Sequential, we have\n",
    "model = torch.nn.Sequential(nn.Linear(1,10),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(10,12),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(12,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `dropout` we have to set the model to train and then to evaluation, since it normalizes scores in training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during training phase, set model to train()\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for x, y in trainloader:\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#then set model to evaluation\n",
    "model.eval()\n",
    "yhat = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights based on the following range\n",
    "\n",
    "$$\\frac{1}{-\\sqrt{L_s}}\\ \\ \\ \\text{and}\\ \\ \\ \\frac{1}{\\sqrt{L_s}}$$\n",
    "\n",
    "where $L_s$ is the boundary. This distribution has the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEQZJREFUeJzt3X+sZGV9x/H3B1bUCBVkr5XuLi6ma3XbNAFvkErT0kIbIA3bplohadSWSkwLbSNpSkPdNvSPRo2akGIttcQfsSDaqluyBq1iTGqhXBQWYUVXqmUDkZUq1hjF1W//mFlzGebuPbM7c8+dx/crmdxz5jx3zvc+d+5nzjznmXtSVUiS2nJM3wVIkqbPcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aENfO964cWNt3bq1r91L0ly66667vl5VC6u16y3ct27dytLSUl+7l6S5lOSrXdo5LCNJDTLcJalBhrskNchwl6QGGe6S1KBVwz3JDUkeTfL5FbYnybVJ9iXZk+SM6ZcpSZpElyP3dwHnH2b7BcC24e0y4O+PvixJ0tFYdZ57VX06ydbDNNkBvKcG1+u7PcmJSU6pqkemVKNadPAgXHstfPObfVcyP44/Hq64Ap75zL4r0RyYxoeYNgEPLVvfP7zvKeGe5DIGR/eceuqpU9i15taePXDllYPlpN9a5sGhax2/5CVw7rn91qK5MI0TquP+Msdedbuqrq+qxapaXFhY9dOzatkPfjD4esst8MMfelvt9pnPPLnfpFVMI9z3A1uWrW8GHp7C40qSjtA0wn0X8KrhrJmzgMcdb5ekfq065p7kRuAcYGOS/cBfAU8DqKp3ALuBC4F9wHeA35tVsZKkbrrMlrlkle0F/NHUKpIkHTU/oSrNkxo7V0F6CsNd/TCkpJky3NUv57h3Yz9pQoa7JDXIcJekBhnuktQgw12SGmS4qx/Oljky9ps6MtwlqUGGu/rlFL9u7CdNyHCXpAYZ7pLUIMNdkhpkuEvzxNky6shwVz8MKWmmDHdJapDhrn45xa8b+0kTMtwlqUGGuyQ1yHCXpAYZ7tI8cZaROjLc1Q9DSpopw12SGmS4q19O8evGftKEDHdJapDhLkkNMtwlqUGGuzRPnGWkjgx39cOQkmbKcFe/nAXSjf2kCXUK9yTnJ3kgyb4kV43ZfmqS25J8LsmeJBdOv1RJUlerhnuSY4HrgAuA7cAlSbaPNPtL4OaqOh24GHj7tAuVJHXX5cj9TGBfVT1YVU8ANwE7RtoU8BPD5WcDD0+vREnSpDZ0aLMJeGjZ+n7gpSNt/hr4WJIrgGcB502lOklP5oloddTlyH3cmZzRZ9glwLuqajNwIfDeJE957CSXJVlKsnTgwIHJq1U7DClpprqE+35gy7L1zTx12OVS4GaAqvpP4BnAxtEHqqrrq2qxqhYXFhaOrGJJ0qq6hPudwLYkpyU5jsEJ010jbf4HOBcgyYsZhLuH5lqdU/y6sZ80oVXDvaoOApcDtwJ7GcyKuS/JNUkuGja7EnhtknuAG4HXVPm+W5L60uWEKlW1G9g9ct/OZcv3A2dPtzRJ0pHyE6qS1CDDXZonjnaqI8Nd/TCkpJky3CWpQYa7+uUUv27sJ03IcJekBhnuktQgw12aJ56IVkeGuyQ1yHBXPzwClWbKcJekBhnu6pdT/LqxnzQhw12SGmS4S1KDDHdpnngiWh0Z7pLUIMNd/fAIVJopw12SGmS4q19O8evGftKEDHdJapDhLs0Tz1WoI8NdkhpkuEtSgwx39cPhBWmmDHf1y1kg3dhPmpDhLkkNMtwlqUGGuzRPPFehjgx3SWqQ4a5+eAQqzZThLkkN6hTuSc5P8kCSfUmuWqHN7yS5P8l9Sf55umWqWU7x68Z+0oQ2rNYgybHAdcCvAfuBO5Psqqr7l7XZBvwFcHZVfSPJc2dVsCRpdV2O3M8E9lXVg1X1BHATsGOkzWuB66rqGwBV9eh0y5QkTaJLuG8CHlq2vn9433IvBF6Y5D+S3J7k/HEPlOSyJEtJlg4cOHBkFUs/zjwRrY66hPu4wb7RZ9gGYBtwDnAJ8M4kJz7lm6qur6rFqlpcWFiYtFZJUkddwn0/sGXZ+mbg4TFtPlJV36+q/wYeYBD20ngegUoz1SXc7wS2JTktyXHAxcCukTYfBn4FIMlGBsM0D06zUElSd6uGe1UdBC4HbgX2AjdX1X1Jrkly0bDZrcBjSe4HbgP+rKoem1XRaohT/LqxnzShVadCAlTVbmD3yH07ly0X8PrhTZLUMz+hKs0Tz1WoI8NdkhpkuEtSgwx39cPhBWmmDHdJapDhrn45xa8b+0kTMtwlqUGGuzRPPFehjgx3SWqQ4S5JDTLc1Q+HF6SZMtwlqUGGu/rlFL9u7CdNyHCX5onDWerIcJekBhnuktQgw12SGmS4qx+OHUszZbirX84C6cZ+0oQMd0lqkOEuzROHs9SR4S5JDTLcJalBhrv64fCCNFOGuyQ1yHBXv5zi1439pAkZ7tI8cThLHRnuktQgw12SGmS4S1KDOoV7kvOTPJBkX5KrDtPu5UkqyeL0SlSTHDuWZmrVcE9yLHAdcAGwHbgkyfYx7U4A/hi4Y9pFSpIm0+XI/UxgX1U9WFVPADcBO8a0+xvgTcB3p1ifWucUv27sJ02oS7hvAh5atr5/eN+PJDkd2FJVt0yxNkmjHM5SR13Cfdwhw4+eYUmOAd4GXLnqAyWXJVlKsnTgwIHuVUqSJtIl3PcDW5atbwYeXrZ+AvBzwKeSfAU4C9g17qRqVV1fVYtVtbiwsHDkVUuSDqtLuN8JbEtyWpLjgIuBXYc2VtXjVbWxqrZW1VbgduCiqlqaScWSpFWtGu5VdRC4HLgV2AvcXFX3JbkmyUWzLlCNcuxYmqkNXRpV1W5g98h9O1doe87RlyVJOhp+QlX9copfN4f6yXc86shwl6QGGe6S1CDDXZIaZLhLUoMMd/XDE4PSTBnuktQgw139cipkN06F1IQMd0lqkOEuSQ0y3CWpQYa7JDXIcFc/PDEozZThrn45W6Yb+0kTMtyleeI7HnVkuEtSgwx3SWqQ4S5JDTLc1Q/HjqWZMtwlqUGGu/rlFL9u/MdhmpDhLkkNMtwlqUGGuyQ1yHCXpAYZ7uqHJwalmTLcJalBhrv65VTIbpwKqQkZ7pLUIMNdkhpkuEtSgzqFe5LzkzyQZF+Sq8Zsf32S+5PsSfKJJM+ffqmSpK5WDfckxwLXARcA24FLkmwfafY5YLGqfh74IPCmaReqxnhiUJqpLkfuZwL7qurBqnoCuAnYsbxBVd1WVd8Zrt4ObJ5umZIAXxTVWZdw3wQ8tGx9//C+lVwKfHTchiSXJVlKsnTgwIHuVapdToXsxn7ShLqE+7hn1djDhyS/CywCbx63vaqur6rFqlpcWFjoXqUkaSIbOrTZD2xZtr4ZeHi0UZLzgKuBX66q702nPEnSkehy5H4nsC3JaUmOAy4Gdi1vkOR04B+Ai6rq0emXKUmaxKrhXlUHgcuBW4G9wM1VdV+Sa5JcNGz2ZuB44ANJ7k6ya4WHkyStgS7DMlTVbmD3yH07ly2fN+W61DpnfUgz5SdU1S9ngXTjPw7ThAx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe7qh7M+joz9po4Md/XLqZDd2E+akOEuSQ0y3CWpQYa7JDXIcJekBhnu6oezPqSZMtyleeKLojoy3NUvp/h1Yz9pQoa7JDXIcJekBhnuktQgw12SGmS4qx/O+pBmynCX5okviurIcFe/nOLXjf2kCRnuktQgw12SGmS4S1KDDHdJapDhrn446+PI2G/qyHCXpAYZ7uqXU/y6sZ80IcNdkhrUKdyTnJ/kgST7klw1ZvvTk7x/uP2OJFunXagkqbtVwz3JscB1wAXAduCSJNtHml0KfKOqfhp4G/DGaRcqSequy5H7mcC+qnqwqp4AbgJ2jLTZAbx7uPxB4NzEQUJJ6suGDm02AQ8tW98PvHSlNlV1MMnjwMnA16dR5JPccAO85S1Tf1itsW99q+8K5tMb3gBvfWvfVeho7dwJr3zlTHfRJdzHHYGPTrbt0oYklwGXAZx66qkddj3GySfD9tFRIc2lk06CF72o7yrmw6ZNcMUV8MgjfVeiaTjppJnvoku47we2LFvfDDy8Qpv9STYAzwb+d/SBqup64HqAxcXFI/s0xo4dg5v04+SYY+Daa/uuQnOky5j7ncC2JKclOQ64GNg10mYX8Orh8suBT1b5UTpJ6suqR+7DMfTLgVuBY4Ebquq+JNcAS1W1C/gn4L1J9jE4Yr94lkVLkg6vy7AMVbUb2D1y385ly98FXjHd0iRJR8pPqEpSgwx3SWqQ4S5JDTLcJalBhrskNSh9TUdPcgD46hF++0Zm8a8Njp51Tca6Jrdea7OuyRxNXc+vqoXVGvUW7kcjyVJVLfZdxyjrmox1TW691mZdk1mLuhyWkaQGGe6S1KB5Dffr+y5gBdY1Geua3HqtzbomM/O65nLMXZJ0ePN65C5JOoy5CPckb07yhSR7knwoyYkrtDvshbxnUNcrktyX5IdJVjzzneQrSe5NcneSpXVU11r313OSfDzJl4Zfx16xIMkPhn11d5LRfy89zXrW5YXfO9T1miQHlvXRH6xRXTckeTTJ51fYniTXDuvek+SMdVLXOUkeX9ZfO8e1m3JNW5LclmTv8G/xT8a0mW1/VdW6vwG/DmwYLr8ReOOYNscCXwZeABwH3ANsn3FdLwZ+BvgUsHiYdl8BNq5hf61aV0/99SbgquHyVeN+j8Nt316DPlr15wf+EHjHcPli4P3rpK7XAH+3Vs+nZfv9JeAM4PMrbL8Q+CiDK7OdBdyxTuo6B7hljfvqFOCM4fIJwBfH/B5n2l9zceReVR+rqoPD1dsZXA1qVJcLeU+7rr1V9cAs93EkOta15v3Fky+k/m7gN2e8v8NZrxd+7+P30klVfZoxV1hbZgfwnhq4HTgxySnroK41V1WPVNVnh8v/B+xlcK3p5WbaX3MR7iN+n8Gr3ahxF/Ie7cy+FPCxJHcNryO7HvTRXz9ZVY/A4MkPPHeFds9IspTk9iSzegHo8vM/6cLvwKELv89S19/Lbw/fyn8wyZYx2/uwnv8GfyHJPUk+muRn13LHw+G804E7RjbNtL86XaxjLST5d+B5YzZdXVUfGba5GjgIvG/cQ4y576inAnWpq4Ozq+rhJM8FPp7kC8OjjT7rWvP+muBhTh321wuATya5t6q+fLS1jZjahd+nrMs+/w24saq+l+R1DN5d/OqM6+qij/7q4rMMPrL/7SQXAh8Gtq3FjpMcD/wL8KdV9a3RzWO+ZWr9tW7CvarOO9z2JK8GfgM4t4YDViO6XMh76nV1fIyHh18fTfIhBm+9jyrcp1DXmvdXkq8lOaWqHhm+/Xx0hcc41F8PJvkUg6OeaYf71C78vtZ1VdVjy1b/kcF5qPVgJs+po7U8VKtqd5K3J9lYVTP9nzNJnsYg2N9XVf86pslM+2suhmWSnA/8OXBRVX1nhWZdLuS95pI8K8kJh5YZnBwee1Z/jfXRX8svpP5q4CnvMJKclOTpw+WNwNnA/TOoZb1e+H3VukbGZS9iMJ67HuwCXjWcBXIW8PihYbg+JXneoXMlSc5kkHuPHf67jnqfYXBt6b1V9dYVms22v9byDPKR3oB9DMam7h7eDs1g+Clg98jZ5y8yOMq7eg3q+i0Gr77fA74G3DpaF4NZD/cMb/etl7p66q+TgU8AXxp+fc7w/kXgncPllwH3DvvrXuDSGdbzlJ8fuIbBQQTAM4APDJ9//wW8YNZ91LGuvx0+l+4BbgNetEZ13Qg8Anx/+Py6FHgd8Lrh9gDXDeu+l8PMIFvjui5f1l+3Ay9bg5p+kcEQy55luXXhWvaXn1CVpAbNxbCMJGkyhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ36fy5BCRDBzg6XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(-2, 2, 0.0001)\n",
    "y = []\n",
    "for v in X:\n",
    "    if v <= -0.5 or v >= 0.5:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\n",
    "plt.plot(X, y, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights based on the following range\n",
    "\n",
    "$$\\frac{\\sqrt{6}}{-\\sqrt{L_{out} + L_{in}}}\\ \\ \\ \\text{and}\\ \\ \\ \\frac{\\sqrt{6}}{\\sqrt{L_{out} + L_{in}}}$$\n",
    "\n",
    "where $L_{in}$ is the number of input neurons and $L_{out}$ the number of output neurons in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pytorch\n",
    "linear = nn.Linear(input_size, output_size)\n",
    "torch.nn.init.xavier_uniform_(linear.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It draws samples from a uniform distribution within (-limit, limit) where limit is $\\sqrt{\\frac{6}{n_{in}}}$ with $n_{in}$ is the number of input units in the weight tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pytorch\n",
    "linear = nn.Linear(input_size, output_size)\n",
    "torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the traditional Gradient Descent that uses only the gradient of the current step to guide the search, Momentum also accumulates the gradient of the past steps to determine what direction to go. In traditional gradient descent, we update weights as:\n",
    "\n",
    "$$w_2 = - \\alpha \\nabla_{w_1}$$\n",
    "\n",
    "where the weights $w_2$ are weights used in the next iteration and $w_1$ are the weights used in the current iteration. In Momentum, we add a variable to the velocity ($v$) and a variable to the coeficient of friction ($\\mu$), which usually is set to 0.5, 0.9 or 0.95. The update using moment becomes:\n",
    "\n",
    "$$v_2 = \\mu v_1 - \\eta \\nabla_{w_1}$$\n",
    "$$w_2 = w_1 + v_2$$\n",
    "\n",
    "In PyTorch, we add a value to the variable `momentum` in the SGD call as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply batch normalization on the activations before we pass it to the activation function. Consider the following neural network:\n",
    "\n",
    "<img src=\"images/neural_network.svg\" width=\"30%\"/>\n",
    "\n",
    "Suppose we have three neurons in the first hidden layer and four neurons in the second hidden layer in this network. Now let's consider the first layer. In order to apply batch normalization, we calculate the mean and standard deviation or variance for a particular mini batch. We then normalize the outputs, scale and shift them. Finally the output is passed to the activation function. For example, consider our input has the form:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 4 & 5 \\\\\n",
    "3 & 4 & 8 & 1 \\\\\n",
    "2 & 3 & 4 & 5 \\\\\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We calculate the values of $Z^1$ for each neuron ($z_1^1$, $z_2^1$, and $z_3^1$) and obtain the matrix $Z^1$ as:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We than calculate the mean and the standard deviation for all samples. Mean is calculated as\n",
    "\n",
    "$$\\mu_{B,1} = \\frac{1}{M} \\sum_{m=1}^{M} z_m$$\n",
    "\n",
    "Having the mean for the first neuron ($z_1^1$) as\n",
    "\n",
    "$$\\mu_{B,1} = \\frac{1}{4} (1+1+1+1) = 1$$\n",
    "\n",
    "The standard deviation ($\\sigma_{B,1}^2$) is calculated as\n",
    "\n",
    "$$\\sigma_{B,1}^2 = \\frac{1}{M} \\sum_{m=1}^{M} (z_m - \\mu_{B,1})^2$$\n",
    "\n",
    "We repeat this process for all the outputs of the second neuron ($\\mu_{B,2}$ and $\\sigma_{B,2}^2$) and for the third neuron ($\\mu_{B,3}$ and $\\sigma_{B,3}^2$), achieving:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\mu_B \\\\\n",
    "\\sigma^2_B \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0.5 & 0.25 \\\\\n",
    "0 & 0.25 & 0.1875 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we normalize the outputs with\n",
    "\n",
    "$$ x_1 = \\frac{z_1^1 - \\mu_{B,1}}{\\sqrt{\\sigma^2_{B,1} + \\epsilon}}$$\n",
    "\n",
    "Thus, we have the following normalized matrix considering $\\epsilon=0.0001$:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "\\end{bmatrix} \\ \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\  x_n = \\frac{z_n^1 - \\mu_{B,n}}{\\sqrt{\\sigma^2_{B,n} + \\epsilon}}\\ \\ \\ \\ \\ \\ \\  \\rightarrow\\ \\ \\ \\ \\ \\ \\  \\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & -0.6 \\\\\n",
    "0 & -1 & -0.6 \\\\\n",
    "0 & 1 & 1.73 \\\\\n",
    "0 & -1 & -0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We than scale and shift parameters using the following equation\n",
    "\n",
    "$$z^1_n = \\gamma_n^1 z^1_n + \\beta_n^1$$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned in training data. Considering $\\gamma$ and $\\beta$ as\n",
    "\n",
    "$$\n",
    "\\gamma = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1.67 \\\\\n",
    "\\end{bmatrix}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We get the following scores\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{matrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & -1 \\\\\n",
    "1 & -1 & -1 \\\\\n",
    "1 & 1 & 2.9 \\\\\n",
    "1 & -1 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we repeat the process for the second layer. We calculate the mean and standard deviation, normalize the values, apply a second scale and shift parameter, and then we pass it the activation functions. Next, We repeat the process for the next batch, as we did it for $x_1$. So we have the second batch that we denote the matrix of our tensor $x_2$. We calculate a new mean and standard deviation and normalize it accordingly. In short, we repeat the whole process again. For prediction, we use the pre-computed matrices (i.e., generated with training data) of mean and standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, in_size, hidden_1, hidden_2, out_size):\n",
    "        super(NetBatchNorm, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, hidden_1)\n",
    "        self.linear2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.linear3 = nn.Linear(hidden_2, out_size)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.linear1(x)))\n",
    "        x = F.relu(self.bn2(self.linear2(x)))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
