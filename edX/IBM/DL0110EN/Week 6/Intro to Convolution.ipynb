{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Convolution?\n",
    "\n",
    "Suppose we have two images that are identical except one is slightly shifted as illustred below. We can see what happens when we convert both images to 12 dimensional vectors, where the grid locations represent the areas\n",
    "where the intensity values are high.\n",
    "\n",
    "<img src=\"images/convolution.svg\" width=\"70%\"/>\n",
    "\n",
    "As we can see the intensity values are in a totally different location. We understand that the relationship between the pixels are more important than the actual location of the pixels. Thus, we can use convolution since it preserves the spatial relationship. \n",
    "\n",
    "So, basically, to apply convolution we have the image and something called the kernel that we perform the convolution. The resulting of this process is an activation map.\n",
    "\n",
    "<img src=\"images/apply_convolution.svg\" width=\"60%\"/>\n",
    "\n",
    "Analogous to a linear equation, convolution can be performed on a vector or a matrix and we have the parameter $W$, which is known as a kernel, the parameter $b$, which is known as a bias, and the operator known as a convolution as:\n",
    "\n",
    "$$Z = W \\ast X + b$$\n",
    "\n",
    "In PyTorch we can create a convolution object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros(nb_images, nb_channels, width, height)\n",
    "image = torch.zeros(1, 1, 5, 5)\n",
    "image[0,0,:,2] = 1\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4304, -0.3784,  0.6896],\n",
       "          [ 0.4304, -0.3784,  0.6896],\n",
       "          [ 0.4304, -0.3784,  0.6896]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How a convolution works can be seen in the image below:\n",
    "   \n",
    "<a href=\"https://stats.stackexchange.com/questions/199702/1d-convolution-in-neural-networks\">\n",
    "    <img src=\"https://i.stack.imgur.com/SFST9.gif\" width=\"40%\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) usually has the architecture as a variation of the architecture below:\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*-1Pad7loK_dFOUvS.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a simple architeture having only two convolutional layers followed by a ReLU activation function and max pooling. In Pytorch, the code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_1=2, out_2=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=2, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(out_2*7*7, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train and validate, the code keeps the same structure as previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train set and a validation set\n",
    "train_dataset = Data(N_images=1000)\n",
    "validation_dataset = Data(N_images=1000, train=False)\n",
    "model = CNN(2, 1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 100\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "train_loader = torch.utils.data.Dataloader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.Dataloader(dataset=validation_dataset, batch_size=5000)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model(x)\n",
    "        loss = criterion(z, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        z = model(x)\n",
    "        _, yhat = torch.max(z.data, 1)\n",
    "        correct += (yhat == y_test).sum().item()\n",
    "    accuracy = float(correct) /N_test\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a number of [pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html) available in Pytorch. An example of how to use a pre-trained model of ResNet18 is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = model.resnet18(pretrained=True)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.255]\n",
    "\n",
    "transforms_stuff = transforms.Compose(\n",
    "                        [transforms.Resize(224), \n",
    "                         transforms.ToTensor(), \n",
    "                         transforms.Normalize(mean, std)]\n",
    ")\n",
    "\n",
    "train_data = dataset(root='./data', download=True, transform=transforms_stuff)\n",
    "validation_data = dataset(root='./data', split='test', download=True, transform=transforms_stuff)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# change the last fc layer \n",
    "model.fc = nn.Linear(512, 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad], \n",
    "                             lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
