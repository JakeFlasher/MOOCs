{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is reinforcement learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a computational approach to goal directed learning from interaction with the environment, using idealized learning situations. By computational approach we can understand as using algorithms and by goal directed learning that we have a goal that we are trying to achieve. Idealized learning situations means that we are really trying to make things a little more narrow in approach to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Reinforcement Learning Problem\n",
    "\n",
    "In a general RL problem, we have an environment and an agent. The environment exposes the state with the agent consent, and the agent senses that state and then takes an action. The environment processes the action, and\n",
    "then produces two things, a reward, and a new state (*state+1*). And the cycle continues, the agent then senses the new state, produces a new action, and so on.\n",
    "\n",
    "<img src=\"images/reinforcement_learning_problem.svg\" width=\"40%\" />\n",
    "\n",
    "In a historical context, RL uses the idea from the psychology of animal learning, where an animal learns the behaviors not through insight but through the process of trial and error, trying different things at random. From studies of animals, Edward Thorndike created the *law of effect* which said, any behavior that is followed by pleasant consequences is likely to be repeated and any that is followed by unpleasant\n",
    "consequences is likely to be stopped. From this perspective, in RL an agent tries different actions selecting from among them by comparing their consequences (*selectional* aspect), and the selection is associated with a particular situation (*associative* aspect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of Reinforcement Learning\n",
    "\n",
    "In the image below, we have the elements of RL, which are explained as follows:\n",
    "\n",
    "<img src=\"images/elements_of_rl.svg\" width=\"40%\" />\n",
    "\n",
    "The **time step** divides time into discrete steps ($t$), where each step determines a cycle in the environment-agent interaction. The **environment** is what defines the world that the agent interacts with. It has a basic loop that initiates by producing a state and a reward for the agent to sense and process. Then, it accepts an action from the agent and cycles back to produce another state again. \n",
    "\n",
    "The **agent** learns to achieve goals by interacting with the environment, having a loop where it senses the state and the reward from the environment, and then selects an action to pass to the environment. The **state** represents the situation in the environment that the agent is going to make his actions based on. \n",
    "\n",
    "**Reward** ($r_t$) is a scalar value, a floating point number, returned by the environment when the agent selects an action. An **action** is what an agent takes on each time step, which can be discrete as of one of a fixed number of actions like in a video game or continuous like steering the wheel at a certain angle or changing the angle of the gas pedal to feed more or less gas in. \n",
    "\n",
    "A **policy** ($\\pi$) is a mapping from a particular state to an action to take in that state. It can be deterministic, where it is the same action each time, or stochastic, where 70% of the time you take action one, and 30% of the time you take action two.\n",
    "\n",
    "**Value function** measures the goodness of a state in the long run as calculated by an agent. In fact, it is the expected long-term accumulation of reward, starting from state $s$, and following policy $\\pi$. As analogy, the reward is kind of like an immediate pleasure or pain experienced by a person, while the value function represents a more farsighted judgement to the value of a state. Value functions come in two basic flavors: the straight value function ($V^{\\pi}(s)$) that represents the goodness of that state when following policy $\\pi$; and the $Q$ function ($Q^{\\pi}(s,a)$), which is the goodness of a state $s$, first taking an action $a$ and then following a normal policy $\\pi$.\n",
    "\n",
    "In a **model** of the environment, we basically get the transition probability to the next state and the probability of the reward going to that state. There are two kinds of methods related to models: *model-free* methods, where we do not have a model and we have a pure trial-and-error learning experience; and *model-based* methods that are considered a planning kind of learner because you typically do not take actions in the environment. Instead, you just follow what would happen through the model itself to find out what your next state is and what your reward is. \n",
    "\n",
    "Regarding tasks, we have the **episodic** tasks, in which they comes to a natural end and they are typically repeated over and over (*e.g.*, tic-tac-toe and Pac-man game). In a **continuing task**, we would be sensing and controlling at each time step, without a natural endpoint (*e.g.*, air conditioner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of RL\n",
    "\n",
    "- **Tic-tac-toe game**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Tic_tac_toe.svg/800px-Tic_tac_toe.svg.png\" width=\"20%\" align=\"center\" />\n",
    "\n",
    "This is a game for two players, \"X\" and \"O\", who take turns marking the spaces in a 3Ã—3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game.\n",
    "\n",
    "| Elements    | Description                                    |\n",
    "| :---------- | :--------------------------------------------- |\n",
    "| Agent       | \"X\" player                                     |\n",
    "| Environment | \"O\" player, general rules                      |\n",
    "| State       | 9x square occupant: X, O or blank              |\n",
    "| Actions     | 9x place \"X\" in square                         |\n",
    "| Rewards     | At the end of the game: 1=wins, 0=tie, -1=loss |\n",
    "| Task type   | Episodic                                       |\n",
    "\n",
    "\n",
    "- **Smart Thermostat**\n",
    "\n",
    "<img src=\"images/thermostat.svg\" width=\"20%\" align=\"center\" />\n",
    "\n",
    "This is a kind of different than most thermostats. In this example, we just have the current temperature as a readout, and sort of two buttons the user can press. If he is happy for the way things are going, he can press the smiley face, and if he is unhappy, he can press the frown face.\n",
    "\n",
    "| Elements    | Description                                    |\n",
    "| :---------- | :--------------------------------------------- |\n",
    "| Agent       | Air conditioning / Heat control                |\n",
    "| Environment | House and its occupants                        |\n",
    "| State       | Current temperature, day, time                 |\n",
    "| Actions     | Heat, Cool, Off                                |\n",
    "| Rewards     | Smile=+1, Frown=-1, None=0                     |\n",
    "| Task type   | Continuing                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to solving RL problems\n",
    "\n",
    "**Value function methods** are methods where we estimate the value states or state action pairs. Our policy is based on selecting the actions that lead to the largest value states. \n",
    "\n",
    "**Direct policy search** is a method that we model the policy itself. The input is typically a state or something we approximate as a state, and the output is the action we want to take, either a discrete action or a continuous action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Simple Rooms Environment\n",
    "\n",
    "For the exercises, we have a file called `simple_rooms.py`. This file, located in `lab/LabFiles/lib/envs/` folder contains classes to the environment of a simple room. The `Environment` class is provided as an interface. An environment must have some representation of the state of which the agent is interacting with. In addition, an environment must be able to reset itself and step to the next state. These are implemented in both the `reset()` and the `step()` function. The `reset()` function should return the initial state, while the `step()` function should take in an action and at the minimum, return the next state and the `reward()`. The `actions()` function maintains the information of how many type of actions in the environment. This is used in conjunction with the `ActionSpace` class.\n",
    "\n",
    "The `SimpleRoomsEnv` class implements the `Environment` class and examine this in more details. The `SimpleRoomsEnv` is a simple environment of a 4x4 rooms, limited by walls. The initial state has the agent starting at the room on top left corner, with the goal to reach the room at the bottom right corner. The 4x4 rooms are illustred in the image below:\n",
    "\n",
    "<img src=\"https://prod-edxapp.edx-cdn.org/assets/courseware/v1/e93aa646b1bd0e176dd7fee8b765a9b4/asset-v1:Microsoft+DAT257x+1T2018+type@asset+block/SimpleRooms.PNG\" width=\"30%\"/>\n",
    "\n",
    "In this environment, we have four actions that can be performed (0:north, 1:east, 2:west, and 3: south) in the 4x4 environment. So, for each room we have specific transitions. For example, our environment is described as a 0 to 15 ids, as follows:\n",
    "\n",
    "<img src=\"images/environment.svg\" width=\"20%\" />\n",
    "\n",
    "In `id=0`, we have only two possible transitions (go to `id=1` or go to `id=4`). In case we are in `id=1`, we have three possibilities (go to `id=0`, go to `id=2`, or go to `id=5`). It is important to note that the blue room (`id=0`) is our start room and the red room (`id=15`) is our goal room (`reward=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: The CliffWalking Environment\n",
    "\n",
    "The `CliffWalking` environment is a simple environment of a 4x12 tiles as illustred in the image below, which has \"cliffs\" or terminal states on it. The initial state has the agent starting at the tile on bottom left corner (yellow tile), with the goal to reach the tile at the bottom right corner (green tile), avoiding the cliffs (navy tiles) in the process.\n",
    "\n",
    "<img src=\"https://prod-edxapp.edx-cdn.org/assets/courseware/v1/a24018ccbfbee5fb1ef116b70d46450c/asset-v1:Microsoft+DAT257x+1T2019+type@asset+block/CliffWalking.PNG\" width=\"40%\"/>\n",
    "\n",
    "In this environment, you have to avoid the cliff tiles and get to the goal as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
